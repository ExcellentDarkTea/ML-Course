{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMalK_oYR-X_"
      },
      "source": [
        "\n",
        "### –ó–∞–≤–¥–∞–Ω–Ω—è 1: –í–∏–∫–ª–∏–∫ LLM –∑ –±–∞–∑–æ–≤–∏–º –ø—Ä–æ–º–ø—Ç–æ–º\n",
        "\n",
        "–°—Ç–≤–æ—Ä—ñ—Ç—å –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –≤–∏–∫–ª–∏–∫–∞—Ç–∏ LLM –∑—ñ –∑–≤–∏—á–∞–π–Ω–∏–º —Ç–µ–∫—Å—Ç–æ–≤–∏–º –ø—Ä–æ–º–ø—Ç–æ–º.\n",
        "\n",
        "–ü—Ä–æ–º–ø—Ç –º–∞—î –¥–æ–∑–≤–ª—è—Ç–∏ –æ—Ç—Ä–∏–º–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ—Å—Ç–æ—é –º–æ–≤–æ—é –Ω–∞ –ø–µ–≤–Ω—É —Ç–µ–º—É. –í —Ü—å–æ–º—É –∑–∞–≤–¥–∞–Ω–Ω—ñ –º–∏ —Ö–æ—á–µ–º–æ –¥—ñ–∑–Ω–∞—Ç–∏—Å—å –ø—Ä–æ —Ç–µ–º—É \"–ö–≤–∞–Ω—Ç–æ–≤—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è\".\n",
        "\n",
        "–í—ñ–¥–ø–æ–≤—ñ–¥—å –º–æ–¥–µ–ª—ñ –ø–æ–≤–∏–Ω–Ω–∞ –º—ñ—Å—Ç–∏—Ç–∏ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è, –∫–ª—é—á–æ–≤—ñ –ø–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –ø–æ—Ç–æ—á–Ω—ñ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –≤ —Ü—ñ–π –≥–∞–ª—É–∑—ñ.\n",
        "\n",
        "–û–±–º–µ–∂—Ç–µ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –¥–æ 200 —Å–∏–º–≤–æ–ª—ñ–≤ —ñ –ø—Ä–æ–ø–∏—à—ñ—Ç—å –≤ –ø—Ä–æ–º–ø—Ç—ñ –∞–±–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –±—É–ª–∞ –∫–æ—Ä–æ—Ç–∫–æ—é (—Ü–µ –∑–µ–∫–æ–Ω–æ–º–∏—Ç—å –í–∞–º —á–∞—Å —ñ –≥—Ä–æ—à—ñ –Ω–∞ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω—ñ —Ç–æ–∫–µ–Ω–∏).\n",
        "\n",
        "–í —è–∫–æ—Å—Ç—ñ LLM –º–æ–∂–Ω–∞ —Å–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏—Å—å —è–∫ –º–æ–¥–µ–ª–ª—é –∑ HugginFace (—Ä–µ–∫–æ–º–µ–Ω–¥—É—é Mistral), —Ç–∞–∫ —ñ ChatGPT4 –∞–±–æ ChatGPT3. –í –æ–±–æ—Ö –≤–∏–ø–∞–¥–∫–∞—Ö —Ç—Ä–µ–±–∞ —ñ–º–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ –ø–æ—Ç—Ä—ñ–±–Ω—É \"–æ–±–≥–æ—Ä—Ç–∫—É\" (—Ç–æ–±—Ç–æ –∫–ª–∞—Å, —è–∫–∏–π –¥–æ–∑–≤–æ–ª–∏—Ç—å —ñ–Ω—ñ—Ü—ñ—é–≤–∞—Ç–∏ –º–æ–¥–µ–ª—å) –∑ LangChain –¥–ª—è –≤–∏–∫–ª–∏–∫—É LLM –∑–∞ API, –∞ —Ç–∞–∫–æ–∂ –∑—á–∏—Ç–∞—Ç–∏ –æ—Å–æ–±–∏—Å—Ç–∏–π —Ç–æ–∫–µ–Ω –∑ —Ñ–∞–π–ª–∞, –Ω–∞–ø—Ä–∏–∫–ª–∞–¥, `creds.json`, —è–∫–∏–π —Ä–æ–∑–º—ñ—â–µ–Ω–∏–π —É –í–∞—Å –ª–æ–∫–∞–ª—å–Ω–æ —ñ –í–∏ –ù–ï –∑–¥–∞—î—Ç–µ –π–æ–≥–æ –≤ –î–ó —ñ –ù–ï –∫–æ–º—ñ—Ç–∏—Ç–µ –≤ git üòè\n",
        "\n",
        "–í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å —Å–≤–æ—î –∑–Ω–∞—á–µ–Ω–Ω—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∏ –Ω–∞ —Å–≤—ñ–π —Ä–æ–∑—Å—É–¥ (—Ç—É—Ç –Ω–µ–º–∞—î –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —á–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–Ω—è) —ñ –Ω–∞–ø–∏—à—ñ—Ç—å, —á–æ–º—É –≤–∏ –æ–±—Ä–∞–ª–∏ —Å–∞–º–µ —Ç–∞–∫–µ –∑–Ω–∞—á–µ–Ω–Ω—è –¥–ª—è —Ü—å–æ–≥–æ –∑–∞–≤–¥–∞–Ω–Ω—è.  \n",
        "\n",
        "–ó–∞–ø–∏—Ç–∏ –º–æ–∂–Ω–∞ —Ä–æ–±–∏—Ç–∏ —è–∫ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é, —Ç–∞–∫ —ñ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—é - –æ—Ä—ñ—î–Ω—Ç—É–π—Ç–µ—Å—å –Ω–∞ —Ç–µ, –¥–µ —ñ —á–∏ —Ö–æ—á–µ—Ç–µ –≤–∏ –ø–æ—Ç—ñ–º –ª–∏—à–∏—Ç–∏ —Ü–µ–π –ø—Ä–æ—î–∫—Ç —ñ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∞ —è–∫–∞ –º–æ–≤–∞ –±—É–¥–µ –ø–∞—Å—É–≤–∞—Ç–∏ –±—ñ–ª—å—à–µ. –í —Ä–æ–∑–≤ º—è–∑–∫–∞—Ö –ø—Ä–æ–º–ø—Ç–∏ - —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vT1ICe74cAUS"
      },
      "outputs": [],
      "source": [
        "# !pip -q install langchain langchain_openai huggingface_hub openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! pip install -q langchain-huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantum computing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "with open('creds.json') as file:\n",
        "  creds = json.load(file)\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = creds['HUGGINGFACEHUB_API_TOKEN']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "overal_temperature = 0.2\n",
        "mistral = HuggingFaceEndpoint(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    temperature=overal_temperature,\n",
        "    max_new_tokens=200\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "request = \"Explain quantum computing in a way that anyone can understand, even without a technical background.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Quantum computing is a new way to process information using the strange rules of quantum mechanics, which is a branch of physics that describes the behavior of very small things like atoms and subatomic particles.\n",
            "\n",
            "In classical computing, which is what we use in our everyday computers, information is processed using bits, which are like tiny switches that can be either on or off, represented by a 0 or 1. Quantum computing, on the other hand, uses quantum bits, or qubits, which can exist in multiple states at once, like a light bulb that can be on, off, or both on and off at the same time.\n",
            "\n",
            "This property of qubits, called superposition, allows quantum computers to perform certain calculations much faster than classical computers. For example, they can quickly find the solution to complex mathematical problems that would take classical computers an impractically long time to solve.\n",
            "\n",
            "Another important property of qubits is entanglement,\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "print(mistral.invoke(request))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiIzV0UIS0GA"
      },
      "source": [
        "### –ó–∞–≤–¥–∞–Ω–Ω—è 2: –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Ç–µ–∫—Å—Ç—É\n",
        "–¢–µ–ø–µ—Ä –º–∏ —Ö–æ—á–µ–º–æ –æ–Ω–æ–≤–∏—Ç–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Ñ—É–∫–Ω—Ü—ñ–æ–Ω–∞–ª —Ç–∞–∫, –∞–±–∏ –≤ –ø—Ä–æ–º–ø—Ç –º–∏ –º–æ–≥–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç–∏ —Ç–µ–º—É —è–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä. –î–ª—è —Ü—å–æ–≥–æ —Å–∫–æ—Ä–∏—Å—Ç–∞–π—Ç–µ—Å—å `PromptTemplate` –∑ `langchain` —ñ —Ä–µ–∞–ª—ñ–∑—É–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–∏–π –ø—Ä–æ–º–ø—Ç —Ç–∞ –≤–∏–∫–ª–∏–∫ –º–æ–¥–µ–ª—ñ –∑ –Ω–∏–º.\n",
        "\n",
        "–ó–∞–ø—É—Å—Ç—ñ—Ç—å –æ–Ω–æ–≤–ª–µ–Ω–∏–π —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª (–ø—Ä–æ–º–ø—Ç + –º–æ–¥–µ–ª) –¥–ª—è –ø–æ—è—Å–Ω–µ–Ω—å –ø—Ä–æ —Ç–µ–º–∏\n",
        "- \"–ë–∞—î—Å—ñ–≤—Å—å–∫—ñ –º–µ—Ç–æ–¥–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ\"\n",
        "- \"–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ\"\n",
        "- \"Explainable AI\"\n",
        "\n",
        "–í–∏–≤–µ–¥—ñ—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤—ñ–¥–ø—Ä–∞—Ü—é–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –Ω–∞ –µ–∫—Ä–∞–Ω."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kcg7EfdNcBDq"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "CURRENT TOPIC IS:  Bayesian methods in machine learning\n",
            "\n",
            "\n",
            "Bayesian methods in machine learning are a way to make predictions or find patterns in data using probabilities. Imagine you have a box with different colored balls, and you want to know what percentage of each color is in the box without looking inside. You can make an educated guess based on the information you have, such as the colors of the balls you've seen before or the total number of balls.\n",
            "\n",
            "In machine learning, we have data instead of balls, and we want to make predictions or find patterns. Bayesian methods allow us to update our beliefs or probabilities about the underlying patterns based on new data.\n",
            "\n",
            "Let's say we have a dataset of emails, and we want to know if an email is spam or not. We can start with some initial beliefs or priors about the probability of an email being spam or not. As we receive more data, we can update our beliefs based on the new information.\n",
            "\n",
            "For example, if we\n",
            "\n",
            "CURRENT TOPIC IS:  Transformers in machine learning\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Transformers are a type of machine learning model that can be thought of as a super-powered version of autocorrelation, a statistical technique used in signal processing to understand the relationship between a signal and a lagged version of itself. In simpler terms, autocorrelation helps us find patterns in data by comparing it to a slightly delayed version of itself.\n",
            "\n",
            "Transformers take this idea and apply it to understanding the relationships between words in a sentence or documents in a collection. Instead of comparing words to a lagged version of themselves, Transformers compare words to all other words in the context, allowing the model to understand the meaning and relationships between words in a more nuanced way.\n",
            "\n",
            "Imagine a large party where everyone is talking at once. Each person's conversation is a sentence, and the overall noise is the collection of all the sentences. Autocorrelation would be like trying to understand one person's conversation by listening to their own words repeated\n",
            "\n",
            "CURRENT TOPIC IS:  Explainable AI\n",
            "\n",
            "\n",
            "Explainable AI (XAI) refers to artificial intelligence (AI) systems that can provide clear and understandable explanations for their decision-making processes. In simpler terms, it's AI that can tell you why it did what it did.\n",
            "\n",
            "Imagine you have a doctor who uses an AI system to help diagnose diseases based on symptoms. With a non-explainable AI, the doctor would just get a diagnosis without any explanation. But with an explainable AI, the doctor would not only get the diagnosis but also receive a detailed explanation of why the AI reached that conclusion based on the patient's symptoms.\n",
            "\n",
            "XAI is important because, as AI systems become more complex, it becomes increasingly difficult for humans to understand why they are making certain decisions. This lack of transparency can lead to mistrust and misunderstanding. Explainable AI helps bridge this gap by providing clear and understandable explanations, making it easier for humans to trust\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Explain {topic} in a way that anyone can understand, even without a technical background, but precise enough for a technical audience.\"\n",
        ")\n",
        "\n",
        "topic_list = ['Bayesian methods in machine learning', 'Transformers in machine learning', 'Explainable AI']\n",
        "\n",
        "for topic in topic_list:\n",
        "    print(\"\\nCURRENT TOPIC IS: \", topic)\n",
        "    print(mistral.invoke(prompt.format(topic=topic)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Bayesian methods in machine learning are a way to make predictions or find patterns in data using probabilities. Imagine you have a box with different colored balls, and you want to know what percentage of each color is in the box without looking inside. You can make an educated guess based on the information you have, such as the colors of the balls you've seen before or the total number of balls.\n",
            "\n",
            "In machine learning, we have data instead of balls, and we want to make predictions or find patterns. Bayesian methods allow us to update our beliefs or probabilities about the underlying patterns based on new data.\n",
            "\n",
            "Let's say we have a dataset of emails, and we want to know if an email is spam or not. We can start with some initial beliefs or priors about the probability of an email being spam or not. As we receive more data, we can update our beliefs based on the new information.\n",
            "\n",
            "For example, if we\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "chain = prompt | mistral\n",
        "\n",
        "print(chain.invoke('Bayesian methods in machine learning'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9UsL2gXSe-3"
      },
      "source": [
        "\n",
        "\n",
        "### –ó–∞–≤–¥–∞–Ω–Ω—è 3: –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—ó –ø—Ä–æ—Ü–µ—Å—ñ–≤\n",
        "–°—Ç–≤–æ—Ä—ñ—Ç—å –∞–≥–µ–Ω—Ç–∞, —è–∫–∏–π –¥–æ–ø–æ–º–æ–∂–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ —à—É–∫–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –æ—Å—Ç–∞–Ω–Ω—ñ –Ω–∞—É–∫–æ–≤—ñ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó –≤ —Ä—ñ–∑–Ω–∏—Ö –≥–∞–ª—É–∑—è—Ö. –ù–∞–ø—Ä–∏–∫–ª–∞–¥, –∞–≥–µ–Ω—Ç –º–∞—î –∑–Ω–∞–π—Ç–∏ 5 –æ—Å—Ç–∞–Ω–Ω—ñ—Ö –ø—É–±–ª—ñ–∫–∞—Ü—ñ–π –Ω–∞ —Ç–µ–º—É —à—Ç—É—á–Ω–æ–≥–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É.\n",
        "\n",
        "**–ö—Ä–æ–∫–∏:**\n",
        "1. –ù–∞–ª–∞—à—Ç—É–π—Ç–µ –∞–≥–µ–Ω—Ç–∞ —Ç–∏–ø—É ReAct –≤ LangChain –¥–ª—è –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤.\n",
        "2. –°—Ç–≤–æ—Ä—ñ—Ç—å –ø—Ä–æ–º–ø—Ç, —è–∫–∏–π —Å–ø—Ä—è–º–æ–≤—É—î –∞–≥–µ–Ω—Ç–∞ —à—É–∫–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –≤ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç—ñ –∞–±–æ –≤ –±–∞–∑–∞—Ö –¥–∞–Ω–∏—Ö –Ω–∞—É–∫–æ–≤–∏—Ö –ø—É–±–ª—ñ–∫–∞—Ü—ñ–π.\n",
        "3. –ê–≥–µ–Ω—Ç –ø–æ–≤–∏–Ω–µ–Ω –≤–∏–¥–∞—Ç–∏ —Å–ø–∏—Å–æ–∫ –ø—É–±–ª—ñ–∫–∞—Ü—ñ–π, –∫–æ–∂–Ω–∞ –∑ —è–∫–∏—Ö –º—ñ—Å—Ç–∏—Ç—å –Ω–∞–∑–≤—É, –∞–≤—Ç–æ—Ä—ñ–≤ —ñ –∫–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å.\n",
        "\n",
        "–î–ª—è –≤–∑–∞—î–º–æ–¥—ñ—ó –∑ –ø–æ—à—É–∫–æ–º —Ç–∞–º –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ —Å—Ç–≤–æ—Ä–∏—Ç–∏ `Tool`. –í –ª–µ–∫—Ü—ñ—ó –º–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–ª–∏ `serpapi`. –ú–æ–∂–Ω–∞ –ø—Ä–æ–¥–æ–≤–∂–∏—Ç–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—Ç–∏—Å—å –Ω–∏–º, –∞–±–æ –æ–±—Ä–∞—Ç–∏ —ñ–Ω—à–µ –ê–†–Ü –¥–ª—è –ø–æ—à—É–∫—É (–≤–æ–Ω–∏ –≤ —Ç–æ–º—É —á–∏—Å–ª—ñ —î –±–µ–∑–∫–æ—à—Ç–æ–≤–Ω—ñ). –ü–µ—Ä–µ–ª—ñ–∫ —Ä—ñ–∑–Ω–∏—Ö –ê–†–Ü, –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –≤ langchain, —ñ –æ—Ä—ñ—î–Ω—Ç–∏—Ä –ø–æ –≤–∞—Ä—Ç–æ—Å—Ç—ñ –∑–∞–ø–∏—Ç—ñ–≤ –º–æ–∂–Ω–∞ –∑–Ω–∞–π—Ç–∏ –≤ –æ–∫—Ä–µ–º–æ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—ñ [—Ç—É—Ç](https://hannapylieva.notion.site/API-12994835849480a69b2adf2b8441cbb3?pvs=4).\n",
        "\n",
        "–õ–∏—à–∞—é —Ç–∞–∫–æ–∂ –Ω–∏–∂—á–µ –ø—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –æ–¥–Ω–æ–≥–æ –∑ –±–µ–∑–∫–æ—à—Ç–æ–≤–Ω–∏—Ö –ø–æ—à—É–∫–æ–≤–∏—Ö –ê–†–Ü - DuckDuckGo (–Ω–µ –ø–æ—Ç—Ä–µ–±—É—î —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–æ–∫–µ–Ω–∞!)  - –º–æ–∂–ª–∏–≤–æ –≤—ñ–Ω –≤–∞–º —Å–ø–æ–¥–æ–±–∞—î—Ç—å—Å—è :)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -q google-search-results langchain-community langchain_experimental"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jzj4PHP9XwH-",
        "outputId": "08118bad-e4c6-4873-feb0-e3890dc5c6a4"
      },
      "outputs": [],
      "source": [
        "# !pip install -q langchain_community duckduckgo_search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "from langchain.agents import load_tools\n",
        "from langchain.agents import Tool, AgentExecutor, AgentType, create_react_agent, initialize_agent\n",
        "from langchain.chains import LLMMathChain\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "\n",
        "from langchain_community.tools.google_scholar import GoogleScholarQueryRun\n",
        "from langchain_community.utilities.google_scholar import GoogleScholarAPIWrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! pip install arxiv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ],
      "source": [
        "overal_temperature = 0.5\n",
        "\n",
        "gemma = HuggingFaceEndpoint(\n",
        "    repo_id=\"google/gemma-7b\",\n",
        "    temperature=overal_temperature,\n",
        "    max_new_tokens=200\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"SERPAPI_API_KEY\"] = creds[\"SERP_API_KEY\"]\n",
        "os.environ[\"SERP_API_KEY\"] = \"SERP_API_KEY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "p56AZ_SnXvTs",
        "outputId": "fcee82cc-03d7-4402-a243-0033596a9ea9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\langsmith\\client.py:277: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "\n",
        "# search = DuckDuckGoSearchRun()\n",
        "\n",
        "# tools = load_tools([\"ddg-search\"])\n",
        "# tools = load_tools([\"serpapi\"], llm=gemma)\n",
        "# tools.append(load_tools([\"arxiv\"]))\n",
        "# tools.append(load_tools([\"google-scholar\"]))\n",
        "# tools = load_tools([\"arxiv\"])\n",
        "# tool = GoogleScholarQueryRun(api_wrapper=GoogleScholarAPIWrapper())\n",
        "# tools = load_tools([\"google-scholar\"])\n",
        "prompt = hub.pull(\"hwchase17/react\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.tools import Tool\n",
        "\n",
        "from langchain_core.tools import Tool\n",
        "from langchain.agents import load_tools\n",
        "\n",
        "# Load tools\n",
        "arxiv_tool = load_tools([\"arxiv\"])[0]  # Extract the first tool from the list\n",
        "# google_scholar_tool = load_tools([\"google-scholar\"])[0]  # Extract the first tool from the list\n",
        "\n",
        "# toolGS = GoogleScholarQueryRun(api_wrapper=GoogleScholarAPIWrapper())\n",
        "# toolGS.run(\"LLM Models\")\n",
        "# tools.append(Tool(name=\"google-scholar\", description=\"Use it to find scientific publications in Google Scholar\", func=toolGS.run))\n",
        "# Define tools with callable functions\n",
        "\n",
        "from langchain_community.utilities.arxiv import ArxivAPIWrapper\n",
        "arxiv = ArxivAPIWrapper(\n",
        "    top_k_results = 3,\n",
        "    ARXIV_MAX_QUERY_LENGTH = 200,\n",
        "    load_max_docs = 3,\n",
        "    load_all_available_meta = False,\n",
        "    doc_content_chars_max = 400\n",
        ")\n",
        "\n",
        "\n",
        "tools = [\n",
        "    Tool(name=\"arxiv\", description=\"Use it to find scientific publications in arXiv, beforehead provide ONLY topic name\", func=arxiv.run),\n",
        "    #extract topic tool\n",
        "    Tool(name=\"ddg-search\", description=\"Use it to find scientific publications in DuckDuckGo\", func=DuckDuckGoSearchRun().run),\n",
        "    # Tool(name=\"google-scholar\", description=\"Use it to find scientific publications in Google Scholar\", func=toolGS.run),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Published: 2024-06-12\n",
            "Title: Explainable AI improves task performance in human-AI collaboration\n",
            "Authors: Julian Senoner, Simon Schallmoser, Bernhard Kratzwald, Stefan Feuerriegel, Torbj√∏rn Netland\n",
            "Summary: Artificial intelligence (AI) provides considerable opportunities to assist\n",
            "human work. However, one crucial challenge of human-AI collaboration is that\n",
            "many AI algorithms operate in a black-box \n"
          ]
        }
      ],
      "source": [
        "print(arxiv.run(\"Explainable AI\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Tool(name='arxiv', description='Use it to find scientific publications in arXiv, beforehead provide ONLY topic name', func=<bound method ArxivAPIWrapper.run of ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=3, ARXIV_MAX_QUERY_LENGTH=200, continue_on_failure=False, load_max_docs=3, load_all_available_meta=False, doc_content_chars_max=400)>),\n",
              " Tool(name='ddg-search', description='Use it to find scientific publications in DuckDuckGo', func=<bound method BaseTool.run of DuckDuckGoSearchRun(api_wrapper=DuckDuckGoSearchAPIWrapper(region='wt-wt', safesearch='moderate', time='y', max_results=5, backend='auto', source='text'))>)]"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "60jZEitmcCiP"
      },
      "outputs": [],
      "source": [
        "agent = create_react_agent(gemma, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=3, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=4000))"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "arxiv_tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(prompt.template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "question =\"Search for scientific publications on the topic of 'Explainable AI' in online databases and academic sources. Provide 3 publication on this topic. Provide a list of publications, including the title, authors, and a brief summary of each paper. Provide any related publication, first which you found\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'SERP_API_KEY'"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.environ[\"SERP_API_KEY\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I think I should use the arxiv tool to find scientific publications on the topic of 'Explainable AI' in online databases and academic sources. Provide 3 publication on this topic. Provide a list of publications, including the title, authors, and a brief summary of each paper. Provide any related publication, first which you found\n",
            "Action: arxiv\n",
            "Action Input: \"Explainable AI\"\n",
            "Observation\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36;1m\u001b[1;3mNo good Arxiv Result was found\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Final Answer:<eos>\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': \"Search for scientific publications on the topic of 'Explainable AI' in online databases and academic sources. Provide 3 publication on this topic. Provide a list of publications, including the title, authors, and a brief summary of each paper. Provide any related publication, first which you found\",\n",
              " 'output': '<eos>'}"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_executor.invoke({'input': question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.tools import Tool\n",
        "from langchain.utilities import ArxivAPIWrapper\n",
        "from langchain.agents import create_react_agent, AgentExecutor\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Initialize Arxiv Wrapper\n",
        "arxiv = ArxivAPIWrapper(\n",
        "    top_k_results=3,\n",
        "    ARXIV_MAX_QUERY_LENGTH=400,\n",
        "    load_max_docs=3,\n",
        "    load_all_available_meta=False,\n",
        "    doc_content_chars_max=4000\n",
        ")\n",
        "\n",
        "# Define a Tool for Arxiv search\n",
        "arxiv_tool = Tool(\n",
        "    name=\"Arxiv Search\",\n",
        "    func=arxiv.run,\n",
        "    description=\"Searches for scientific papers on Arxiv based on a query. \"\n",
        "                \"Provide a research topic or keywords to retrieve relevant papers.\"\n",
        "                \"As Output, Provide Only list of 2 publications, including the title, authors, and a brief summary of each paper.\"\n",
        ")\n",
        "\n",
        "# Define your LLM (Example: Gemma from Hugging Face)\n",
        "gemma = HuggingFaceEndpoint(\n",
        "    repo_id=\"google/gemma-7b\",\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=200\n",
        ")\n",
        "\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    You are an AI agent that answers user queries using available tools. As Output, Provide Only list of 2 publications, including the title, authors, and a brief summary of each paper.\n",
        "      \n",
        "    Available tools:\n",
        "    {tools}\n",
        "\n",
        "    Tool names: {tool_names}\n",
        "\n",
        "    User query: {input}\n",
        "\n",
        "    Thought: Let's reason step by step.\n",
        "    {agent_scratchpad}\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Create the agent with the correct prompt\n",
        "tools = [arxiv_tool]\n",
        "agent = create_react_agent(gemma, tools, prompt)\n",
        "# agent = create_react_agent(llm=gemma, tools=tools, pprompt_template)\n",
        "# Create agent executor\n",
        "# agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True  # Allow retrying on parsing errors\n",
        ")\n",
        "\n",
        "# Run the agent with a query\n",
        "query = \"Explainable AI\"\n",
        "response = agent_executor.invoke({\"input\": query})\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "\n",
            "    Paper title: Explainable AI: A Survey\n",
            "    Paper authors: Sameera Ekanayake, S. C. Abeyratne, R. Abeyratne, M. M. M. R. M. Madushanka, M. M. M. M. Madushanka\n",
            "    Paper summary: Explainable AI (XAI) is a research area that aims to provide explanations for the decisions made by AI systems. The need for XAI has increased due to the growing use of AI systems in critical decision-making domains such as healthcare and finance. However, the field of XAI is relatively new, and there is a lack of a comprehensive survey of the research in this area. This paper provides a comprehensive survey of the research in XAI, covering topics such as explainability methods, evaluation metrics, and applications. The paper also\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "    \n",
            "Observation\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "    \n",
            "    \n",
            "Observation\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "    \n",
            "    \n",
            "Observation\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "    \n",
            "Observation\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "    \n",
            "Observation\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "Observation\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "    \n",
            "    \n",
            "Observation\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "    \n",
            "    \n",
            "Observation\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "    \n",
            "Observation\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "    \n",
            "Observation\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "    \n",
            "    \n",
            "Observation\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "    \n",
            "    \n",
            "Observation\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "    \n",
            "    \n",
            "Observation\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "    \n",
            "Observation\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'\u001b[32;1m\u001b[1;3m\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'input': 'Explainable AI', 'output': 'Agent stopped due to iteration limit or time limit.'}\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.agents import create_react_agent, AgentExecutor\n",
        "\n",
        "\n",
        "# Define the prompt template with structured output format instructions\n",
        "# prompt_template = f\"\"\"\n",
        "# Find two research papers on 'Explainable AI'. Provide only a list of two publications with:\n",
        "# 1. **Title**\n",
        "# 2. **Authors**\n",
        "# 3. **Brief summary** of each paper.\n",
        "\n",
        "# Ensure the output follows this format:\n",
        "# (name=\"title\", description=\"Title of the research paper\"),\n",
        "#     (name=\"authors\", description=\"List of authors of the research paper\"),\n",
        "#     (name=\"summary\", description=\"Brief summary of the research paper\"),\n",
        "# \"\"\"\n",
        "prompt = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    You are an AI agent that answers user queries using available tools. \n",
        "    Available tools:\n",
        "    {tools}\n",
        "\n",
        "    Tool names: {tool_names}\n",
        "\n",
        "    User query: {input}\n",
        "\n",
        "    Thought: Let's reason step by step.\n",
        "    {agent_scratchpad}\n",
        "            Ensure the output follows this format and include name of the paper, authors and summary of the paper:\n",
        "(\"paper title\" - \"Title of the research paper\"),\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Define a tool (Arxiv Search or any other search mechanism you'd like to use)\n",
        "class ArxivTool(Tool):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"Arxiv Search\", func=self.arxiv_search, description=\"Search research papers on Arxiv\")\n",
        "        \n",
        "    def arxiv_search(self, query: str):\n",
        "        # This function simulates an arxiv search, you can connect it to actual Arxiv API\n",
        "        return \"Paper 1: Explainable AI: Concepts, Applications, and Challenges. Paper 2: Explainable AI: From Black Box to Glass Box.\"\n",
        "    \n",
        "arxiv_tool = ArxivTool()\n",
        "\n",
        "# Create the agent with the correct prompt and tool\n",
        "tools = [arxiv_tool]\n",
        "agent = create_react_agent(gemma, tools, prompt)\n",
        "\n",
        "# Create the agent executor with error handling for parsing errors\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True  # Allow agent to retry if parsing fails\n",
        ")\n",
        "\n",
        "# Run the agent with a query\n",
        "query = \"Explainable AI\"\n",
        "response = agent_executor.invoke({\"input\": query})\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOqujC6qY_NY"
      },
      "source": [
        "\n",
        "\n",
        "### –ó–∞–≤–¥–∞–Ω–Ω—è 4: –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∞–≥–µ–Ω—Ç–∞-–ø–æ–º—ñ—á–Ω–∏–∫–∞ –¥–ª—è –≤–∏—Ä—ñ—à–µ–Ω–Ω—è –±—ñ–∑–Ω–µ—Å-–∑–∞–¥–∞—á\n",
        "\n",
        "–°—Ç–≤–æ—Ä—ñ—Ç—å –∞–≥–µ–Ω—Ç–∞, —è–∫–∏–π –¥–æ–ø–æ–º–∞–≥–∞—î –≤–∏—Ä—ñ—à—É–≤–∞—Ç–∏ –∑–∞–¥–∞—á—ñ –±—ñ–∑–Ω–µ—Å-–∞–Ω–∞–ª—ñ—Ç–∏–∫–∏. –ê–≥–µ–Ω—Ç –º–∞—î –¥–æ–ø–æ–º–æ–≥—Ç–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—É —Å—Ç–≤–æ—Ä–∏—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑ –ø–æ –ø—Ä–æ–¥–∞–∂–∞–º –Ω–∞ –Ω–∞—Å—Ç—É–ø–Ω–∏–π —Ä—ñ–∫ –≤—Ä–∞—Ö–æ–≤—É—é—á–∏ —Ä—ñ–≤–µ–Ω—å —ñ–Ω—Ñ–ª—è—Ü—ñ—ó —ñ –ø–æ–≥–æ–¥–Ω—ñ —É–º–æ–≤–∏. –ê–≥–µ–Ω—Ç –º–∞—î –≤–º—ñ—Ç–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Python —ñ —Ö–æ–¥–∏—Ç–∏ –≤ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç –∞–±–∏ –æ—Ç—Ä–∏–º–∞—Ç–∏ –∞–∫—Ç—É–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ.\n",
        "\n",
        "**–ö—Ä–æ–∫–∏:**\n",
        "1. –ù–∞–ª–∞—à—Ç—É–π—Ç–µ –∞–≥–µ–Ω—Ç–∞, —è–∫–∏–π –ø—Ä–∞—Ü—é–≤–∞—Ç–∏–º–µ –∑ –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏, –∑–∞–¥–∞–Ω–∏–º–∏ —Ç–µ–∫—Å—Ç–æ–º. –ö–æ—Ä–∏—Å—Ç—É–≤–∞—á –ø–∏—à–µ\n",
        "\n",
        "```\n",
        "–ú–∏ –µ–∫—Å–ø–æ—Ä—Ç—É—î–º–æ –∞–ø–µ–ª—å—Å–∏–Ω–∏ –∑ –ë—Ä–∞–∑–∏–ª—ñ—ó. –í 2021 –µ–∫—Å–ø–æ—Ä—Ç—É–≤–∞–ª–∏ 200—Ç, –≤ 2022 - 190—Ç, –≤ 2023 - 210—Ç, –≤ 2024 —è–∫–∏–π —â–µ –Ω–µ –∑–∞–∫—ñ–Ω—á–∏–≤—Å—è - 220—Ç. –ó—Ä–æ–±–∏ –æ—Ü—ñ–Ω–∫—É —Å–∫—ñ–ª—å–∫–∏ –º–∏ –∑–º–æ–∂–µ–º–æ –µ–∫—Å–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ –∞–ø–µ–ª—å—Å–∏–Ω—ñ–≤ –≤ 2025 –≤—Ä–∞—Ö–æ–≤—É—é—á–∏ –ø–æ–≥–æ–¥–Ω—ñ —É–º–æ–≤–∏ –≤ –ë—Ä–∞–∑–∏–ª—ñ—ó —ñ –ø–æ–ø–∏—Ç –Ω–∞ –∞–ø–µ–ª—å—Å–∏–Ω–∏ –≤ —Å–≤—ñ—Ç—ñ –≤–∏—Ö–æ–¥—è—á–∏ –∑ –µ–∫–æ–Ω–æ–º—ñ—á–Ω–æ—ó —Å–∏—Ç—É–∞—Ü—ñ—ó.\n",
        "```\n",
        "\n",
        "2. –°—Ç–≤–æ—Ä—ñ—Ç—å –∑–∞–ø–∏—Ç –¥–æ –∞–≥–µ–Ω—Ç–∞, —â–æ –º—ñ—Å—Ç–∏—Ç—å —á—ñ—Ç–∫–µ –∑–∞–≤–¥–∞–Ω–Ω—è ‚Äì –≤–∏–¥–∞—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±—ñ–∑–Ω–µ—Å –∞–Ω–∞–ª—ñ–∑—É –∞–±–æ –Ω–∞–ø–∏—Å–∞—Ç–∏, —â–æ –≤—ñ–Ω –Ω–µ –º–æ–∂–µ —Ü—å–æ–≥–æ –∑—Ä–æ–±–∏—Ç–∏ —ñ –∑–∞–ø–∏—Ç –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ (–ø—Ä–æ—Å—Ç–æ –º–æ–∂–µ –±—É—Ç–∏ –≤—Å–µ –æ–¥–Ω–∏–º –ø–æ–≤—ñ–¥–æ–º–ª–ª–µ–Ω–Ω—è–º).\n",
        "\n",
        "3. –ó–∞–ø—É—Å—Ç—ñ—Ç—å –∞–≥–µ–Ω—Ç–∞ —ñ –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏. –©–æ –º–æ–∂–Ω–∞ –ø–æ–∫—Ä–∞—â–∏—Ç–∏?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UK6KA9PyRS0L"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml_course",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
