{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMalK_oYR-X_"
      },
      "source": [
        "\n",
        "### –ó–∞–≤–¥–∞–Ω–Ω—è 1: –í–∏–∫–ª–∏–∫ LLM –∑ –±–∞–∑–æ–≤–∏–º –ø—Ä–æ–º–ø—Ç–æ–º\n",
        "\n",
        "–°—Ç–≤–æ—Ä—ñ—Ç—å –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –≤–∏–∫–ª–∏–∫–∞—Ç–∏ LLM –∑—ñ –∑–≤–∏—á–∞–π–Ω–∏–º —Ç–µ–∫—Å—Ç–æ–≤–∏–º –ø—Ä–æ–º–ø—Ç–æ–º.\n",
        "\n",
        "–ü—Ä–æ–º–ø—Ç –º–∞—î –¥–æ–∑–≤–ª—è—Ç–∏ –æ—Ç—Ä–∏–º–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ—Å—Ç–æ—é –º–æ–≤–æ—é –Ω–∞ –ø–µ–≤–Ω—É —Ç–µ–º—É. –í —Ü—å–æ–º—É –∑–∞–≤–¥–∞–Ω–Ω—ñ –º–∏ —Ö–æ—á–µ–º–æ –¥—ñ–∑–Ω–∞—Ç–∏—Å—å –ø—Ä–æ —Ç–µ–º—É \"–ö–≤–∞–Ω—Ç–æ–≤—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è\".\n",
        "\n",
        "–í—ñ–¥–ø–æ–≤—ñ–¥—å –º–æ–¥–µ–ª—ñ –ø–æ–≤–∏–Ω–Ω–∞ –º—ñ—Å—Ç–∏—Ç–∏ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è, –∫–ª—é—á–æ–≤—ñ –ø–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –ø–æ—Ç–æ—á–Ω—ñ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –≤ —Ü—ñ–π –≥–∞–ª—É–∑—ñ.\n",
        "\n",
        "–û–±–º–µ–∂—Ç–µ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –¥–æ 200 —Å–∏–º–≤–æ–ª—ñ–≤ —ñ –ø—Ä–æ–ø–∏—à—ñ—Ç—å –≤ –ø—Ä–æ–º–ø—Ç—ñ –∞–±–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –±—É–ª–∞ –∫–æ—Ä–æ—Ç–∫–æ—é (—Ü–µ –∑–µ–∫–æ–Ω–æ–º–∏—Ç—å –í–∞–º —á–∞—Å —ñ –≥—Ä–æ—à—ñ –Ω–∞ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω—ñ —Ç–æ–∫–µ–Ω–∏).\n",
        "\n",
        "–í —è–∫–æ—Å—Ç—ñ LLM –º–æ–∂–Ω–∞ —Å–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏—Å—å —è–∫ –º–æ–¥–µ–ª–ª—é –∑ HugginFace (—Ä–µ–∫–æ–º–µ–Ω–¥—É—é Mistral), —Ç–∞–∫ —ñ ChatGPT4 –∞–±–æ ChatGPT3. –í –æ–±–æ—Ö –≤–∏–ø–∞–¥–∫–∞—Ö —Ç—Ä–µ–±–∞ —ñ–º–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ –ø–æ—Ç—Ä—ñ–±–Ω—É \"–æ–±–≥–æ—Ä—Ç–∫—É\" (—Ç–æ–±—Ç–æ –∫–ª–∞—Å, —è–∫–∏–π –¥–æ–∑–≤–æ–ª–∏—Ç—å —ñ–Ω—ñ—Ü—ñ—é–≤–∞—Ç–∏ –º–æ–¥–µ–ª—å) –∑ LangChain –¥–ª—è –≤–∏–∫–ª–∏–∫—É LLM –∑–∞ API, –∞ —Ç–∞–∫–æ–∂ –∑—á–∏—Ç–∞—Ç–∏ –æ—Å–æ–±–∏—Å—Ç–∏–π —Ç–æ–∫–µ–Ω –∑ —Ñ–∞–π–ª–∞, –Ω–∞–ø—Ä–∏–∫–ª–∞–¥, `creds.json`, —è–∫–∏–π —Ä–æ–∑–º—ñ—â–µ–Ω–∏–π —É –í–∞—Å –ª–æ–∫–∞–ª—å–Ω–æ —ñ –í–∏ –ù–ï –∑–¥–∞—î—Ç–µ –π–æ–≥–æ –≤ –î–ó —ñ –ù–ï –∫–æ–º—ñ—Ç–∏—Ç–µ –≤ git üòè\n",
        "\n",
        "–í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å —Å–≤–æ—î –∑–Ω–∞—á–µ–Ω–Ω—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∏ –Ω–∞ —Å–≤—ñ–π —Ä–æ–∑—Å—É–¥ (—Ç—É—Ç –Ω–µ–º–∞—î –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —á–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–Ω—è) —ñ –Ω–∞–ø–∏—à—ñ—Ç—å, —á–æ–º—É –≤–∏ –æ–±—Ä–∞–ª–∏ —Å–∞–º–µ —Ç–∞–∫–µ –∑–Ω–∞—á–µ–Ω–Ω—è –¥–ª—è —Ü—å–æ–≥–æ –∑–∞–≤–¥–∞–Ω–Ω—è.  \n",
        "\n",
        "–ó–∞–ø–∏—Ç–∏ –º–æ–∂–Ω–∞ —Ä–æ–±–∏—Ç–∏ —è–∫ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é, —Ç–∞–∫ —ñ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—é - –æ—Ä—ñ—î–Ω—Ç—É–π—Ç–µ—Å—å –Ω–∞ —Ç–µ, –¥–µ —ñ —á–∏ —Ö–æ—á–µ—Ç–µ –≤–∏ –ø–æ—Ç—ñ–º –ª–∏—à–∏—Ç–∏ —Ü–µ–π –ø—Ä–æ—î–∫—Ç —ñ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∞ —è–∫–∞ –º–æ–≤–∞ –±—É–¥–µ –ø–∞—Å—É–≤–∞—Ç–∏ –±—ñ–ª—å—à–µ. –í —Ä–æ–∑–≤ º—è–∑–∫–∞—Ö –ø—Ä–æ–º–ø—Ç–∏ - —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vT1ICe74cAUS"
      },
      "outputs": [],
      "source": [
        "# !pip -q install langchain langchain_openai huggingface_hub openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! pip install -q langchain-huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantum computing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "with open('creds.json') as file:\n",
        "  creds = json.load(file)\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = creds['HUGGINGFACEHUB_API_TOKEN']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "overal_temperature = 0.2\n",
        "mistral = HuggingFaceEndpoint(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    temperature=overal_temperature,\n",
        "    max_new_tokens=200\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "request = \"Explain quantum computing in a way that anyone can understand, even without a technical background.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Quantum computing is a new way to process information using the strange and counterintuitive rules of quantum mechanics. In our everyday life, we use classical computers that work with bits, which are like tiny switches that can be either on or off. Quantum computers, on the other hand, use quantum bits, or qubits, which can exist in multiple states at once.\n",
            "\n",
            "Imagine a coin that can be heads or tails, but also spin at the same time, making it heads and tails at the same time in different directions. This might sound confusing, but it's the key to quantum computing's power. With qubits, quantum computers can process a vast number of possibilities all at once, making them incredibly fast for certain types of calculations.\n",
            "\n",
            "Classical computers are like a single person trying to solve a complex puzzle by looking at one piece at a time. Quantum computers, however, are like a team of people, each looking\n"
          ]
        }
      ],
      "source": [
        "print(mistral.invoke(request))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiIzV0UIS0GA"
      },
      "source": [
        "### –ó–∞–≤–¥–∞–Ω–Ω—è 2: –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Ç–µ–∫—Å—Ç—É\n",
        "–¢–µ–ø–µ—Ä –º–∏ —Ö–æ—á–µ–º–æ –æ–Ω–æ–≤–∏—Ç–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Ñ—É–∫–Ω—Ü—ñ–æ–Ω–∞–ª —Ç–∞–∫, –∞–±–∏ –≤ –ø—Ä–æ–º–ø—Ç –º–∏ –º–æ–≥–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç–∏ —Ç–µ–º—É —è–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä. –î–ª—è —Ü—å–æ–≥–æ —Å–∫–æ—Ä–∏—Å—Ç–∞–π—Ç–µ—Å—å `PromptTemplate` –∑ `langchain` —ñ —Ä–µ–∞–ª—ñ–∑—É–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–∏–π –ø—Ä–æ–º–ø—Ç —Ç–∞ –≤–∏–∫–ª–∏–∫ –º–æ–¥–µ–ª—ñ –∑ –Ω–∏–º.\n",
        "\n",
        "–ó–∞–ø—É—Å—Ç—ñ—Ç—å –æ–Ω–æ–≤–ª–µ–Ω–∏–π —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª (–ø—Ä–æ–º–ø—Ç + –º–æ–¥–µ–ª) –¥–ª—è –ø–æ—è—Å–Ω–µ–Ω—å –ø—Ä–æ —Ç–µ–º–∏\n",
        "- \"–ë–∞—î—Å—ñ–≤—Å—å–∫—ñ –º–µ—Ç–æ–¥–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ\"\n",
        "- \"–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ\"\n",
        "- \"Explainable AI\"\n",
        "\n",
        "–í–∏–≤–µ–¥—ñ—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤—ñ–¥–ø—Ä–∞—Ü—é–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –Ω–∞ –µ–∫—Ä–∞–Ω."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kcg7EfdNcBDq"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "CURRENT TOPIC IS:  Bayesian methods in machine learning\n",
            "\n",
            "\n",
            "Bayesian methods in machine learning are a way to make predictions or find patterns in data using probabilities. Imagine you have a box with different colored balls, and you want to know what percentage of each color is in the box without looking inside. You can make an educated guess based on the information you have, such as the colors of the balls you've seen before or the total number of balls.\n",
            "\n",
            "In machine learning, we have data instead of balls, and we want to make predictions or find patterns. Bayesian methods allow us to update our beliefs or probabilities about the underlying patterns based on new data.\n",
            "\n",
            "Let's say we have a dataset of emails, and we want to know if an email is spam or not. We can start with some initial beliefs or priors about the probability of an email being spam or not. As we receive more data, we can update our beliefs based on the new information.\n",
            "\n",
            "For example, if we\n",
            "\n",
            "CURRENT TOPIC IS:  Transformers in machine learning\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Transformers are a type of machine learning model that can be thought of as a super-powered version of autocorrelation, a statistical technique used in signal processing to understand the relationship between a signal and a lagged version of itself. In simpler terms, autocorrelation helps us find patterns in data by comparing it to a slightly delayed version of itself.\n",
            "\n",
            "Transformers take this idea and apply it to understanding the relationships between words in a sentence or documents in a collection. Instead of comparing words to a lagged version of themselves, Transformers compare words to all other words in the context, allowing the model to understand the meaning and relationships between words in a more nuanced way.\n",
            "\n",
            "Imagine a large party where everyone is talking at once. Each person's conversation is a sentence, and the overall noise is the collection of all the sentences. Autocorrelation would be like trying to understand one person's conversation by listening to their own words repeated\n",
            "\n",
            "CURRENT TOPIC IS:  Explainable AI\n",
            "\n",
            "\n",
            "Explainable AI (XAI) refers to artificial intelligence (AI) systems that can provide clear and understandable explanations for their decision-making processes. In simpler terms, it's AI that can tell you why it did what it did.\n",
            "\n",
            "Imagine you have a doctor who uses an AI system to help diagnose diseases based on symptoms. With a non-explainable AI, the doctor would just get a diagnosis without any explanation. But with an explainable AI, the doctor would not only get the diagnosis but also receive a detailed explanation of why the AI reached that conclusion based on the patient's symptoms.\n",
            "\n",
            "XAI is important because, as AI systems become more complex, it becomes increasingly difficult for humans to understand why they are making certain decisions. This lack of transparency can lead to mistrust and misunderstanding. Explainable AI helps bridge this gap by providing clear and understandable explanations, making it easier for humans to trust\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Explain {topic} in a way that anyone can understand, even without a technical background, but precise enough for a technical audience.\"\n",
        ")\n",
        "\n",
        "topic_list = ['Bayesian methods in machine learning', 'Transformers in machine learning', 'Explainable AI']\n",
        "\n",
        "for topic in topic_list:\n",
        "    print(\"\\nCURRENT TOPIC IS: \", topic)\n",
        "    print(mistral.invoke(prompt.format(topic=topic)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Bayesian methods in machine learning are a way to make predictions or find patterns in data using probabilities. Imagine you have a box with different colored balls, and you want to know what percentage of each color is in the box without looking inside. You can make an educated guess based on the information you have, such as the colors of the balls you've seen before or the total number of balls.\n",
            "\n",
            "In machine learning, we have data instead of balls, and we want to make predictions or find patterns. Bayesian methods allow us to update our beliefs or probabilities about the underlying patterns based on new data.\n",
            "\n",
            "Let's say we have a dataset of emails, and we want to know if an email is spam or not. We can start with some initial beliefs or priors about the probability of an email being spam or not. As we receive more data, we can update our beliefs based on the new information.\n",
            "\n",
            "For example, if we\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "chain = prompt | mistral\n",
        "\n",
        "print(chain.invoke('Bayesian methods in machine learning'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9UsL2gXSe-3"
      },
      "source": [
        "\n",
        "\n",
        "### –ó–∞–≤–¥–∞–Ω–Ω—è 3: –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—ó –ø—Ä–æ—Ü–µ—Å—ñ–≤\n",
        "–°—Ç–≤–æ—Ä—ñ—Ç—å –∞–≥–µ–Ω—Ç–∞, —è–∫–∏–π –¥–æ–ø–æ–º–æ–∂–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ —à—É–∫–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –æ—Å—Ç–∞–Ω–Ω—ñ –Ω–∞—É–∫–æ–≤—ñ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó –≤ —Ä—ñ–∑–Ω–∏—Ö –≥–∞–ª—É–∑—è—Ö. –ù–∞–ø—Ä–∏–∫–ª–∞–¥, –∞–≥–µ–Ω—Ç –º–∞—î –∑–Ω–∞–π—Ç–∏ 5 –æ—Å—Ç–∞–Ω–Ω—ñ—Ö –ø—É–±–ª—ñ–∫–∞—Ü—ñ–π –Ω–∞ —Ç–µ–º—É —à—Ç—É—á–Ω–æ–≥–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É.\n",
        "\n",
        "**–ö—Ä–æ–∫–∏:**\n",
        "1. –ù–∞–ª–∞—à—Ç—É–π—Ç–µ –∞–≥–µ–Ω—Ç–∞ —Ç–∏–ø—É ReAct –≤ LangChain –¥–ª—è –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤.\n",
        "2. –°—Ç–≤–æ—Ä—ñ—Ç—å –ø—Ä–æ–º–ø—Ç, —è–∫–∏–π —Å–ø—Ä—è–º–æ–≤—É—î –∞–≥–µ–Ω—Ç–∞ —à—É–∫–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –≤ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç—ñ –∞–±–æ –≤ –±–∞–∑–∞—Ö –¥–∞–Ω–∏—Ö –Ω–∞—É–∫–æ–≤–∏—Ö –ø—É–±–ª—ñ–∫–∞—Ü—ñ–π.\n",
        "3. –ê–≥–µ–Ω—Ç –ø–æ–≤–∏–Ω–µ–Ω –≤–∏–¥–∞—Ç–∏ —Å–ø–∏—Å–æ–∫ –ø—É–±–ª—ñ–∫–∞—Ü—ñ–π, –∫–æ–∂–Ω–∞ –∑ —è–∫–∏—Ö –º—ñ—Å—Ç–∏—Ç—å –Ω–∞–∑–≤—É, –∞–≤—Ç–æ—Ä—ñ–≤ —ñ –∫–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å.\n",
        "\n",
        "–î–ª—è –≤–∑–∞—î–º–æ–¥—ñ—ó –∑ –ø–æ—à—É–∫–æ–º —Ç–∞–º –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ —Å—Ç–≤–æ—Ä–∏—Ç–∏ `Tool`. –í –ª–µ–∫—Ü—ñ—ó –º–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–ª–∏ `serpapi`. –ú–æ–∂–Ω–∞ –ø—Ä–æ–¥–æ–≤–∂–∏—Ç–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—Ç–∏—Å—å –Ω–∏–º, –∞–±–æ –æ–±—Ä–∞—Ç–∏ —ñ–Ω—à–µ –ê–†–Ü –¥–ª—è –ø–æ—à—É–∫—É (–≤–æ–Ω–∏ –≤ —Ç–æ–º—É —á–∏—Å–ª—ñ —î –±–µ–∑–∫–æ—à—Ç–æ–≤–Ω—ñ). –ü–µ—Ä–µ–ª—ñ–∫ —Ä—ñ–∑–Ω–∏—Ö –ê–†–Ü, –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –≤ langchain, —ñ –æ—Ä—ñ—î–Ω—Ç–∏—Ä –ø–æ –≤–∞—Ä—Ç–æ—Å—Ç—ñ –∑–∞–ø–∏—Ç—ñ–≤ –º–æ–∂–Ω–∞ –∑–Ω–∞–π—Ç–∏ –≤ –æ–∫—Ä–µ–º–æ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—ñ [—Ç—É—Ç](https://hannapylieva.notion.site/API-12994835849480a69b2adf2b8441cbb3?pvs=4).\n",
        "\n",
        "–õ–∏—à–∞—é —Ç–∞–∫–æ–∂ –Ω–∏–∂—á–µ –ø—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –æ–¥–Ω–æ–≥–æ –∑ –±–µ–∑–∫–æ—à—Ç–æ–≤–Ω–∏—Ö –ø–æ—à—É–∫–æ–≤–∏—Ö –ê–†–Ü - DuckDuckGo (–Ω–µ –ø–æ—Ç—Ä–µ–±—É—î —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–æ–∫–µ–Ω–∞!)  - –º–æ–∂–ª–∏–≤–æ –≤—ñ–Ω –≤–∞–º —Å–ø–æ–¥–æ–±–∞—î—Ç—å—Å—è :)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -q google-search-results langchain-community langchain_experimental"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jzj4PHP9XwH-",
        "outputId": "08118bad-e4c6-4873-feb0-e3890dc5c6a4"
      },
      "outputs": [],
      "source": [
        "# !pip install -q langchain_community duckduckgo_search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "from langchain.agents import load_tools\n",
        "from langchain.agents import Tool, AgentExecutor, AgentType, create_react_agent, initialize_agent\n",
        "from langchain.chains import LLMMathChain\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "\n",
        "from langchain_community.tools.google_scholar import GoogleScholarQueryRun\n",
        "from langchain_community.utilities.google_scholar import GoogleScholarAPIWrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! pip install arxiv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ],
      "source": [
        "overal_temperature = 0.2\n",
        "\n",
        "gemma = HuggingFaceEndpoint(\n",
        "    repo_id= \"mistralai/Mistral-7B-Instruct-v0.2\", #\"google/gemma-7b\",\n",
        "    temperature=overal_temperature,\n",
        "    max_new_tokens=200\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('creds.json') as file:\n",
        "  creds = json.load(file)\n",
        "os.environ[\"OPENAI_API_KEY\"] = creds[\"OPENAI_API_KEY\"]\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"SERPAPI_API_KEY\"] = creds[\"SERPAPI_API_KEY\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "p56AZ_SnXvTs",
        "outputId": "fcee82cc-03d7-4402-a243-0033596a9ea9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langsmith\\client.py:277: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "\n",
        "tools = load_tools([\"ddg-search\"], llm=llm)  # DuckDuckGo search tool\n",
        "tools.extend(load_tools([\"arxiv\"], llm=llm))  # Arxiv search tool\n",
        "\n",
        "prompt = hub.pull(\"hwchase17/react\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[DuckDuckGoSearchRun(api_wrapper=DuckDuckGoSearchAPIWrapper(region='wt-wt', safesearch='moderate', time='y', max_results=5, backend='auto', source='text')),\n",
              " ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=3, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=4000))]"
            ]
          },
          "execution_count": 143,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent = create_react_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo find recent publications on the topic of 'Artificial Intelligence', I will search for relevant papers on arXiv, as it is a comprehensive source for scientific articles in this field.\n",
            "\n",
            "Action: arxiv\n",
            "Action Input: Artificial Intelligence\u001b[0m\u001b[33;1m\u001b[1;3mPublished: 2023-04-06\n",
            "Title: The Governance of Physical Artificial Intelligence\n",
            "Authors: Yingbo Li, Anamaria-Beatrice Spulber, Yucong Duan\n",
            "Summary: Physical artificial intelligence can prove to be one of the most important\n",
            "challenges of the artificial intelligence. The governance of physical\n",
            "artificial intelligence would define its responsible intelligent application in\n",
            "the society.\n",
            "\n",
            "Published: 2020-05-21\n",
            "Title: Does an artificial intelligence perform market manipulation with its own discretion? -- A genetic algorithm learns in an artificial market simulation\n",
            "Authors: Takanobu Mizuta\n",
            "Summary: Who should be charged with responsibility for an artificial intelligence\n",
            "performing market manipulation have been discussed. In this study, I\n",
            "constructed an artificial intelligence using a genetic algorithm that learns in\n",
            "an artificial market simulation, and investigated whether the artificial\n",
            "intelligence discovers market manipulation through learning with an artificial\n",
            "market simulation despite a builder of artificial intelligence has no intention\n",
            "of market manipulation. As a result, the artificial intelligence discovered\n",
            "market manipulation as an optimal investment strategy. This result suggests\n",
            "necessity of regulation, such as obligating builders of artificial intelligence\n",
            "to prevent artificial intelligence from performing market manipulation.\n",
            "\n",
            "Published: 2015-07-01\n",
            "Title: Impact of Artificial Intelligence on Economic Theory\n",
            "Authors: Tshilidzi Marwala\n",
            "Summary: Artificial intelligence has impacted many aspects of human life. This paper\n",
            "studies the impact of artificial intelligence on economic theory. In particular\n",
            "we study the impact of artificial intelligence on the theory of bounded\n",
            "rationality, efficient market hypothesis and prospect theory.\u001b[0m\u001b[32;1m\u001b[1;3mI have found three relevant publications on the topic of 'Artificial Intelligence' from arXiv. To provide a more comprehensive list, I will search for additional recent publications using DuckDuckGo.\n",
            "\n",
            "Action: duckduckgo_search\n",
            "Action Input: recent publications on Artificial Intelligence 2023\u001b[0m\u001b[36;1m\u001b[1;3mFind the latest research papers and news in Artificial Intelligence. Read stories and opinions from top researchers in our research community. ... Artificial Intelligence. Uncover the latest and most impactful research in Artificial Intelligence. Explore pioneering discoveries, insightful ideas and new methods from leading researchers in the ... In the new report, foundation models dominate, benchmarks fall, prices skyrocket, and on the global stage, the U.S. overshadows. \"Data Page: Annual scholarly publications on artificial intelligence\", part of the following publication: Charlie Giattino, Edouard Mathieu, Veronika Samborska and Max Roser (2023) - \"Artificial Intelligence\". Data adapted from Center for Security and Emerging Technology. Between 2013 and 2023, the scientific literature on artificial intelligence grew at an exponential rate. The subject appears to be gaining even greater popularity across a wide range of fields. Although interest in the area is now at its peak, there is a scarcity of comprehensive bibliometric study. As a result, a new perspective on the constantly changing accomplishments, contributions, and ... The most recent edition of the \"UN Activities on AI Report\" represents a collaborative endeavor between the International Telecommunication Union (ITU) and 46 United Nations agencies and bodies. This comprehensive report showcases 408 artificial intelligence (AI) cases and projects run by the UN system, encompassing all 17 Sustainable Development Goals (SDGs). The initiatives range from ...\u001b[0m\u001b[32;1m\u001b[1;3mI have found three relevant publications on the topic of 'Artificial Intelligence' from arXiv. To complete the list, I will summarize the findings and provide a list of five publications, including the title, authors, and a brief summary of each paper.\n",
            "\n",
            "1. **Title:** The Governance of Physical Artificial Intelligence  \n",
            "   **Authors:** Yingbo Li, Anamaria-Beatrice Spulber, Yucong Duan  \n",
            "   **Summary:** This paper discusses the governance of physical artificial intelligence, emphasizing its importance as a challenge in AI. It explores how governance can ensure responsible and intelligent application in society.  \n",
            "   [Link to paper](https://arxiv.org/abs/2304.006)\n",
            "\n",
            "2. **Title:** Does an artificial intelligence perform market manipulation with its own discretion? -- A genetic algorithm learns in an artificial market simulation  \n",
            "   **Authors:** Takanobu Mizuta  \n",
            "   **Summary:** The study investigates whether AI can discover market manipulation strategies through learning in an artificial market simulation. It highlights the need for regulation to prevent AI from engaging in market manipulation.  \n",
            "   [Link to paper](https://arxiv.org/abs/2005.021)\n",
            "\n",
            "3. **Title:** Impact of Artificial Intelligence on Economic Theory  \n",
            "   **Authors:** Tshilidzi Marwala  \n",
            "   **Summary:** This paper examines the impact of AI on economic theory, focusing on bounded rationality, efficient market hypothesis, and prospect theory. It discusses how AI influences these economic concepts.  \n",
            "   [Link to paper](https://arxiv.org/abs/1507.001)\n",
            "\n",
            "4. **Title:** UN Activities on AI Report  \n",
            "   **Authors:** International Telecommunication Union (ITU) and 46 United Nations agencies  \n",
            "   **Summary:** This report showcases 408 AI cases and projects run by the UN system, covering all 17 Sustainable Development Goals (SDGs). It highlights the collaborative efforts of the UN in utilizing AI for global development.  \n",
            "   [Link to report](https://www.itu.int/en/ITU-T/AI/Pages/default.aspx)\n",
            "\n",
            "5. **Title:** Data Page: Annual scholarly publications on artificial intelligence  \n",
            "   **Authors:** Charlie Giattino, Edouard Mathieu, Veronika Samborska, Max Roser  \n",
            "   **Summary:** This publication provides a bibliometric study of the exponential growth in AI literature from 2013 to 2023. It offers insights into the increasing popularity and contributions of AI research across various fields.  \n",
            "   [Link to publication](https://ourworldindata.org/artificial-intelligence)\n",
            "\n",
            "These publications provide a diverse overview of recent research and developments in the field of Artificial Intelligence.\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'\u001b[32;1m\u001b[1;3mI have compiled a list of five publications on the topic of 'Artificial Intelligence', including titles, authors, and brief summaries.\n",
            "\n",
            "Final Answer:\n",
            "\n",
            "1. **Title:** The Governance of Physical Artificial Intelligence  \n",
            "   **Authors:** Yingbo Li, Anamaria-Beatrice Spulber, Yucong Duan  \n",
            "   **Summary:** This paper discusses the governance of physical artificial intelligence, emphasizing its importance as a challenge in AI. It explores how governance can ensure responsible and intelligent application in society.  \n",
            "   [Link to paper](https://arxiv.org/abs/2304.006)\n",
            "\n",
            "2. **Title:** Does an artificial intelligence perform market manipulation with its own discretion? -- A genetic algorithm learns in an artificial market simulation  \n",
            "   **Authors:** Takanobu Mizuta  \n",
            "   **Summary:** The study investigates whether AI can discover market manipulation strategies through learning in an artificial market simulation. It highlights the need for regulation to prevent AI from engaging in market manipulation.  \n",
            "   [Link to paper](https://arxiv.org/abs/2005.021)\n",
            "\n",
            "3. **Title:** Impact of Artificial Intelligence on Economic Theory  \n",
            "   **Authors:** Tshilidzi Marwala  \n",
            "   **Summary:** This paper examines the impact of AI on economic theory, focusing on bounded rationality, efficient market hypothesis, and prospect theory. It discusses how AI influences these economic concepts.  \n",
            "   [Link to paper](https://arxiv.org/abs/1507.001)\n",
            "\n",
            "4. **Title:** UN Activities on AI Report  \n",
            "   **Authors:** International Telecommunication Union (ITU) and 46 United Nations agencies  \n",
            "   **Summary:** This report showcases 408 AI cases and projects run by the UN system, covering all 17 Sustainable Development Goals (SDGs). It highlights the collaborative efforts of the UN in utilizing AI for global development.  \n",
            "   [Link to report](https://www.itu.int/en/ITU-T/AI/Pages/default.aspx)\n",
            "\n",
            "5. **Title:** Data Page: Annual scholarly publications on artificial intelligence  \n",
            "   **Authors:** Charlie Giattino, Edouard Mathieu, Veronika Samborska, Max Roser  \n",
            "   **Summary:** This publication provides a bibliometric study of the exponential growth in AI literature from 2013 to 2023. It offers insights into the increasing popularity and contributions of AI research across various fields.  \n",
            "   [Link to publication](https://ourworldindata.org/artificial-intelligence)\n",
            "\n",
            "These publications provide a diverse overview of recent research and developments in the field of Artificial Intelligence.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': \"Search for publications, papers, or news on the topic of 'Artificial Intelligence'. Provide 5 publications on this topic. Provide a list of publications, including the title, authors, and a brief summary of each paper (link if possible).\",\n",
              " 'output': '1. **Title:** The Governance of Physical Artificial Intelligence  \\n   **Authors:** Yingbo Li, Anamaria-Beatrice Spulber, Yucong Duan  \\n   **Summary:** This paper discusses the governance of physical artificial intelligence, emphasizing its importance as a challenge in AI. It explores how governance can ensure responsible and intelligent application in society.  \\n   [Link to paper](https://arxiv.org/abs/2304.006)\\n\\n2. **Title:** Does an artificial intelligence perform market manipulation with its own discretion? -- A genetic algorithm learns in an artificial market simulation  \\n   **Authors:** Takanobu Mizuta  \\n   **Summary:** The study investigates whether AI can discover market manipulation strategies through learning in an artificial market simulation. It highlights the need for regulation to prevent AI from engaging in market manipulation.  \\n   [Link to paper](https://arxiv.org/abs/2005.021)\\n\\n3. **Title:** Impact of Artificial Intelligence on Economic Theory  \\n   **Authors:** Tshilidzi Marwala  \\n   **Summary:** This paper examines the impact of AI on economic theory, focusing on bounded rationality, efficient market hypothesis, and prospect theory. It discusses how AI influences these economic concepts.  \\n   [Link to paper](https://arxiv.org/abs/1507.001)\\n\\n4. **Title:** UN Activities on AI Report  \\n   **Authors:** International Telecommunication Union (ITU) and 46 United Nations agencies  \\n   **Summary:** This report showcases 408 AI cases and projects run by the UN system, covering all 17 Sustainable Development Goals (SDGs). It highlights the collaborative efforts of the UN in utilizing AI for global development.  \\n   [Link to report](https://www.itu.int/en/ITU-T/AI/Pages/default.aspx)\\n\\n5. **Title:** Data Page: Annual scholarly publications on artificial intelligence  \\n   **Authors:** Charlie Giattino, Edouard Mathieu, Veronika Samborska, Max Roser  \\n   **Summary:** This publication provides a bibliometric study of the exponential growth in AI literature from 2013 to 2023. It offers insights into the increasing popularity and contributions of AI research across various fields.  \\n   [Link to publication](https://ourworldindata.org/artificial-intelligence)\\n\\nThese publications provide a diverse overview of recent research and developments in the field of Artificial Intelligence.'}"
            ]
          },
          "execution_count": 147,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Invoke the agent with the input\n",
        "response = agent_executor.invoke({\n",
        "    'input': \"Search for publications, papers, or news on the topic of 'Artificial Intelligence'. Provide 5 publications on this topic. Provide a list of publications, including the title, authors, and a brief summary of each paper (link if possible).\"\n",
        "})\n",
        "\n",
        "# Print the response\n",
        "save_response = response\n",
        "save_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Does an artificial intelligence perform market manipulation with its own discretion? -- A genetic algorithm learns in an artificial market simulation \n",
        "https://arxiv.org/abs/2005.10488 -> https://arxiv.org/abs/2005.021\n",
        "\n",
        "The Governance of Physical Artificial Intelligence  \n",
        "https://arxiv.org/abs/2304.02924 -> https://arxiv.org/abs/2304.006"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input': \"Search for publications, papers, or news on the topic of 'Artificial Intelligence'. Provide 5 publications on this topic. Provide a list of publications, including the title, authors, and a brief summary of each paper (link if possible).\", 'output': '1. **Title:** The Governance of Physical Artificial Intelligence  \\n   **Authors:** Yingbo Li, Anamaria-Beatrice Spulber, Yucong Duan  \\n   **Summary:** This paper discusses the governance of physical artificial intelligence, emphasizing its importance as a challenge in AI. It explores how governance can ensure responsible and intelligent application in society.  \\n   [Link to paper](https://arxiv.org/abs/2304.006)\\n\\n2. **Title:** Does an artificial intelligence perform market manipulation with its own discretion? -- A genetic algorithm learns in an artificial market simulation  \\n   **Authors:** Takanobu Mizuta  \\n   **Summary:** The study investigates whether AI can discover market manipulation strategies through learning in an artificial market simulation. It highlights the need for regulation to prevent AI from engaging in market manipulation.  \\n   [Link to paper](https://arxiv.org/abs/2005.021)\\n\\n3. **Title:** Impact of Artificial Intelligence on Economic Theory  \\n   **Authors:** Tshilidzi Marwala  \\n   **Summary:** This paper examines the impact of AI on economic theory, focusing on bounded rationality, efficient market hypothesis, and prospect theory. It discusses how AI influences these economic concepts.  \\n   [Link to paper](https://arxiv.org/abs/1507.001)\\n\\n4. **Title:** UN Activities on AI Report  \\n   **Authors:** International Telecommunication Union (ITU) and 46 United Nations agencies  \\n   **Summary:** This report showcases 408 AI cases and projects run by the UN system, covering all 17 Sustainable Development Goals (SDGs). It highlights the collaborative efforts of the UN in utilizing AI for global development.  \\n   [Link to report](https://www.itu.int/en/ITU-T/AI/Pages/default.aspx)\\n\\n5. **Title:** Data Page: Annual scholarly publications on artificial intelligence  \\n   **Authors:** Charlie Giattino, Edouard Mathieu, Veronika Samborska, Max Roser  \\n   **Summary:** This publication provides a bibliometric study of the exponential growth in AI literature from 2013 to 2023. It offers insights into the increasing popularity and contributions of AI research across various fields.  \\n   [Link to publication](https://ourworldindata.org/artificial-intelligence)\\n\\nThese publications provide a diverse overview of recent research and developments in the field of Artificial Intelligence.'}\n"
          ]
        }
      ],
      "source": [
        "print(save_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('creds.json') as file:\n",
        "  creds = json.load(file)\n",
        "os.environ[\"OPENAI_API_KEY\"] = creds[\"OPENAI_API_KEY\"]\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langsmith\\client.py:277: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[72], line 57\u001b[0m\n\u001b[0;32m     48\u001b[0m agent_executor \u001b[38;5;241m=\u001b[39m AgentExecutor(\n\u001b[0;32m     49\u001b[0m     agent\u001b[38;5;241m=\u001b[39magent,\n\u001b[0;32m     50\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     53\u001b[0m     handle_parsing_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     54\u001b[0m )\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Example execution\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m response \u001b[38;5;241m=\u001b[39m agent_executor\u001b[38;5;241m.\u001b[39minvoke({\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArtificial Intelligence\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     59\u001b[0m })\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain\\chains\\base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain\\chains\\base.py:160\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 160\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    166\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    167\u001b[0m     )\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain\\agents\\agent.py:1624\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m   1622\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m-> 1624\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_take_next_step(\n\u001b[0;32m   1625\u001b[0m         name_to_tool_map,\n\u001b[0;32m   1626\u001b[0m         color_mapping,\n\u001b[0;32m   1627\u001b[0m         inputs,\n\u001b[0;32m   1628\u001b[0m         intermediate_steps,\n\u001b[0;32m   1629\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m   1630\u001b[0m     )\n\u001b[0;32m   1631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m   1632\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[0;32m   1633\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[0;32m   1634\u001b[0m         )\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain\\agents\\agent.py:1332\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1322\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1323\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1327\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1328\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m   1330\u001b[0m         [\n\u001b[0;32m   1331\u001b[0m             a\n\u001b[1;32m-> 1332\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[0;32m   1333\u001b[0m                 name_to_tool_map,\n\u001b[0;32m   1334\u001b[0m                 color_mapping,\n\u001b[0;32m   1335\u001b[0m                 inputs,\n\u001b[0;32m   1336\u001b[0m                 intermediate_steps,\n\u001b[0;32m   1337\u001b[0m                 run_manager,\n\u001b[0;32m   1338\u001b[0m             )\n\u001b[0;32m   1339\u001b[0m         ]\n\u001b[0;32m   1340\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain\\agents\\agent.py:1358\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1355\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[1;32m-> 1358\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_agent\u001b[38;5;241m.\u001b[39mplan(\n\u001b[0;32m   1359\u001b[0m         intermediate_steps,\n\u001b[0;32m   1360\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1361\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[0;32m   1362\u001b[0m     )\n\u001b[0;32m   1363\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1364\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain\\agents\\agent.py:465\u001b[0m, in \u001b[0;36mRunnableAgent.plan\u001b[1;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    457\u001b[0m final_output: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_runnable:\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;66;03m# Use streaming to make sure that the underlying LLM is invoked in a\u001b[39;00m\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;66;03m# streaming\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;66;03m# Because the response from the plan is not a generator, we need to\u001b[39;00m\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;66;03m# accumulate the output into final output and return that.\u001b[39;00m\n\u001b[1;32m--> 465\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunnable\u001b[38;5;241m.\u001b[39mstream(inputs, config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}):\n\u001b[0;32m    466\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    467\u001b[0m             final_output \u001b[38;5;241m=\u001b[39m chunk\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3409\u001b[0m, in \u001b[0;36mRunnableSequence.stream\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3403\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\n\u001b[0;32m   3404\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3405\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   3406\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3407\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   3408\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 3409\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3396\u001b[0m, in \u001b[0;36mRunnableSequence.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m   3391\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3392\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[0;32m   3393\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3394\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   3395\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 3396\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[0;32m   3397\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3398\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[0;32m   3399\u001b[0m         patch_config(config, run_name\u001b[38;5;241m=\u001b[39m(config \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m   3400\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3401\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2199\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   2197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2198\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 2199\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mnext\u001b[39m, iterator)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   2200\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[0;32m   2201\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3359\u001b[0m, in \u001b[0;36mRunnableSequence._transform\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   3356\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3357\u001b[0m         final_pipeline \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mtransform(final_pipeline, config)\n\u001b[1;32m-> 3359\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m final_pipeline\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1413\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1410\u001b[0m final: Input\n\u001b[0;32m   1411\u001b[0m got_first_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1413\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ichunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[0;32m   1414\u001b[0m     \u001b[38;5;66;03m# The default implementation of transform is to buffer input and\u001b[39;00m\n\u001b[0;32m   1415\u001b[0m     \u001b[38;5;66;03m# then call stream.\u001b[39;00m\n\u001b[0;32m   1416\u001b[0m     \u001b[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001b[39;00m\n\u001b[0;32m   1417\u001b[0m     \u001b[38;5;66;03m# the `+` operator.\u001b[39;00m\n\u001b[0;32m   1418\u001b[0m     \u001b[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001b[39;00m\n\u001b[0;32m   1419\u001b[0m     \u001b[38;5;66;03m# only operate on the last chunk,\u001b[39;00m\n\u001b[0;32m   1420\u001b[0m     \u001b[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001b[39;00m\n\u001b[0;32m   1421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m got_first_val:\n\u001b[0;32m   1422\u001b[0m         final \u001b[38;5;241m=\u001b[39m ichunk\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5565\u001b[0m, in \u001b[0;36mRunnableBindingBase.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m   5560\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5561\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[0;32m   5562\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5563\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   5564\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 5565\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   5566\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   5567\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5568\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   5569\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1431\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1428\u001b[0m             final \u001b[38;5;241m=\u001b[39m ichunk\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[1;32m-> 1431\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(final, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:428\u001b[0m, in \u001b[0;36mBaseChatModel.stream\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrate_limiter\u001b[38;5;241m.\u001b[39macquire(blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 428\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    429\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    430\u001b[0m             chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_manager\u001b[38;5;241m.\u001b[39mrun_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:841\u001b[0m, in \u001b[0;36mBaseChatOpenAI._stream\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    839\u001b[0m         base_generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 841\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload)\n\u001b[0;32m    842\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m response\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\openai\\_utils\\_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:914\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    873\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    911\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    912\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    913\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    915\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    916\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    917\u001b[0m             {\n\u001b[0;32m    918\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    919\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    920\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[0;32m    921\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    922\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    923\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    924\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    925\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    926\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m    927\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    928\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    929\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[0;32m    930\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    931\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    932\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[0;32m    933\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    934\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[0;32m    935\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    936\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    937\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    938\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    939\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m    940\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    941\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m    942\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    943\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    944\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    945\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    946\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    947\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    948\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb_search_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: web_search_options,\n\u001b[0;32m    949\u001b[0m             },\n\u001b[0;32m    950\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    951\u001b[0m         ),\n\u001b[0;32m    952\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    953\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    954\u001b[0m         ),\n\u001b[0;32m    955\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    956\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    957\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    958\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\openai\\_base_client.py:1242\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1230\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1239\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1240\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1241\u001b[0m     )\n\u001b[1;32m-> 1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\openai\\_base_client.py:919\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 919\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    920\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    921\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    922\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    923\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    924\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m    925\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\openai\\_base_client.py:1008\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1007\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1009\u001b[0m         input_options,\n\u001b[0;32m   1010\u001b[0m         cast_to,\n\u001b[0;32m   1011\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1012\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1013\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1014\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1015\u001b[0m     )\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\openai\\_base_client.py:1057\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1057\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1058\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1059\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1060\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1061\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1062\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1063\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\openai\\_base_client.py:1008\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1007\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1009\u001b[0m         input_options,\n\u001b[0;32m   1010\u001b[0m         cast_to,\n\u001b[0;32m   1011\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1012\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1013\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1014\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1015\u001b[0m     )\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\openai\\_base_client.py:1057\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1057\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1058\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1059\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1060\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1061\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1062\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1063\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\openai\\_base_client.py:1023\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1020\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1022\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1026\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1027\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1031\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1032\u001b[0m )\n",
            "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ],
      "source": [
        "from langchain import hub\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "# from langchain_community.tools import TavilySearchResults\n",
        "from langchain_community.utilities.arxiv import ArxivAPIWrapper\n",
        "from langchain_community.utilities import SerpAPIWrapper\n",
        "# from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# 1. Configure tools for different search types\n",
        "search = SerpAPIWrapper()\n",
        "arxiv = ArxivAPIWrapper()\n",
        "# tavily = TavilySearchResults()\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = creds[\"OPENAI_API_KEY\"]\n",
        "gemma = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
        "\n",
        "tools = [\n",
        "    # Tool(\n",
        "    #     name=\"web_search\",\n",
        "    #     description=\"Search internet for recent blogs and news articles\",\n",
        "    #     func=search.run,\n",
        "    # ),\n",
        "    Tool(\n",
        "        name=\"arxiv_search\",\n",
        "        description=\"Search academic publications on Arxiv\",\n",
        "        func=arxiv.run,\n",
        "    ),\n",
        "]\n",
        "\n",
        "# 2. Create customized ReAct prompt\n",
        "custom_prompt = \"\"\"You are an AI agent that searches for recent scientific publications, blogs, news on a given topic using available tools. Always:\n",
        "- Cite sources for factual claims\n",
        "- Return 5 most recent results\n",
        "- Include title, authors, and 1-sentence description\n",
        "\n",
        "Tools available: {tools}\n",
        "\n",
        "Format final answer as:\n",
        "<Publication Type> Results:\n",
        "1. [Title] by [Authors] - [Description]\n",
        "... (repeat for 5 results)\"\"\"\n",
        "\n",
        "# llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "prompt = hub.pull(\"hwchase17/react\").partial(tools=custom_prompt)\n",
        "\n",
        "# 3. Initialize ReAct agent\n",
        "agent = create_react_agent(gemma, tools, prompt)\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    verbose=True,\n",
        "    max_iterations=5,\n",
        "    handle_parsing_errors=True\n",
        ")\n",
        "\n",
        "# Example execution\n",
        "response = agent_executor.invoke({\n",
        "    \"input\": \"Artificial Intelligence\"\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_variables=['agent_scratchpad', 'input', 'tool_names'] input_types={} partial_variables={'tools': 'You are an AI agent that searches for recent scientific publications, blogs, news on a GIVEN topic using available tools. Always:\\n- Cite sources for factual claims\\n- Return 5 most recent results\\n- Include title, authors, and 1-sentence description\\n\\nTools available: {tools}\\n\\nFormat final answer as:\\n<Publication Type> Results:\\n1. [Title] by [Authors] - [Description]\\n... (repeat for 5 results)'} metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'react', 'lc_hub_commit_hash': 'd15fe3c426f1c4b3f37c9198853e4a86e20c425ca7f4752ec0c9b0e97ca7ea4d'} template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}'\n"
          ]
        }
      ],
      "source": [
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `1. Title: \"Deep Learning for Time Series Forecasting: A Review\"\n       Authors: M. S. S. Murthy, S. S. S. Prasad, and S. S. S. Prasanna\n       Description: This paper reviews the recent developments in deep learning techniques for time series forecasting. It discusses various deep learning architectures, their applications, and the challenges in implementing them for time series forecasting.\n\n    2. Title: \"Artificial Intelligence in Healthcare: Current Applications and Future Prospects\"\n       Authors: S. K. Sharma, S. K. Singh, and R. K. Pal\n       Description: This review article discusses the current applications of artificial intelligence (AI) in healthcare and its future prospects. It covers various AI techniques, their applications in healthcare, and the challenges and opportunities in implementing them.\n\n    3. Title: \"Artificial Intelligence in Education:`\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain\\agents\\agent.py:1358\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[1;32m-> 1358\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_agent\u001b[38;5;241m.\u001b[39mplan(\n\u001b[0;32m   1359\u001b[0m         intermediate_steps,\n\u001b[0;32m   1360\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1361\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[0;32m   1362\u001b[0m     )\n\u001b[0;32m   1363\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain\\agents\\agent.py:465\u001b[0m, in \u001b[0;36mRunnableAgent.plan\u001b[1;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_runnable:\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;66;03m# Use streaming to make sure that the underlying LLM is invoked in a\u001b[39;00m\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;66;03m# streaming\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;66;03m# Because the response from the plan is not a generator, we need to\u001b[39;00m\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;66;03m# accumulate the output into final output and return that.\u001b[39;00m\n\u001b[1;32m--> 465\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunnable\u001b[38;5;241m.\u001b[39mstream(inputs, config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}):\n\u001b[0;32m    466\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3409\u001b[0m, in \u001b[0;36mRunnableSequence.stream\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3403\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\n\u001b[0;32m   3404\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3405\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   3406\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3407\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   3408\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 3409\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3396\u001b[0m, in \u001b[0;36mRunnableSequence.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m   3391\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3392\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[0;32m   3393\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3394\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   3395\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 3396\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[0;32m   3397\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3398\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[0;32m   3399\u001b[0m         patch_config(config, run_name\u001b[38;5;241m=\u001b[39m(config \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m   3400\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3401\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2199\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   2198\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 2199\u001b[0m     chunk: Output \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mnext\u001b[39m, iterator)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   2200\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3359\u001b[0m, in \u001b[0;36mRunnableSequence._transform\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   3357\u001b[0m         final_pipeline \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mtransform(final_pipeline, config)\n\u001b[1;32m-> 3359\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m final_pipeline\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1431\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[1;32m-> 1431\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(final, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1000\u001b[0m, in \u001b[0;36mRunnable.stream\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Default implementation of stream, which calls invoke.\u001b[39;00m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;124;03mSubclasses should override this method if they support streaming output.\u001b[39;00m\n\u001b[0;32m    991\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;124;03m    The output of the Runnable.\u001b[39;00m\n\u001b[0;32m    999\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1000\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:202\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result([Generation(text\u001b[38;5;241m=\u001b[39minner_input)]),\n\u001b[0;32m    204\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    205\u001b[0m         config,\n\u001b[0;32m    206\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    207\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1925\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[0;32m   1922\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m   1923\u001b[0m         output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1924\u001b[0m             Output,\n\u001b[1;32m-> 1925\u001b[0m             context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m   1926\u001b[0m                 call_func_with_variable_args,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1927\u001b[0m                 func,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1928\u001b[0m                 \u001b[38;5;28minput\u001b[39m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1929\u001b[0m                 config,\n\u001b[0;32m   1930\u001b[0m                 run_manager,\n\u001b[0;32m   1931\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1932\u001b[0m             ),\n\u001b[0;32m   1933\u001b[0m         )\n\u001b[0;32m   1934\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain_core\\runnables\\config.py:430\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:203\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[1;34m(inner_input)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m--> 203\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result([Generation(text\u001b[38;5;241m=\u001b[39minner_input)]),\n\u001b[0;32m    204\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    205\u001b[0m         config,\n\u001b[0;32m    206\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    207\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:247\u001b[0m, in \u001b[0;36mBaseOutputParser.parse_result\u001b[1;34m(self, result, partial)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Parse a list of candidate model Generations into a specific format.\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \n\u001b[0;32m    235\u001b[0m \u001b[38;5;124;03mThe return value is parsed from only the first Generation in the result, which\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m    Structured output.\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse(result[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext)\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain\\agents\\output_parsers\\react_single_input.py:75\u001b[0m, in \u001b[0;36mReActSingleInputOutputParser.parse\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*:[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*(.*?)\u001b[39m\u001b[38;5;124m\"\u001b[39m, text, re\u001b[38;5;241m.\u001b[39mDOTALL):\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse LLM output: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     77\u001b[0m         observation\u001b[38;5;241m=\u001b[39mMISSING_ACTION_AFTER_THOUGHT_ERROR_MESSAGE,\n\u001b[0;32m     78\u001b[0m         llm_output\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m     79\u001b[0m         send_to_llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     80\u001b[0m     )\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*Action\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*Input\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*:[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*(.*)\u001b[39m\u001b[38;5;124m\"\u001b[39m, text, re\u001b[38;5;241m.\u001b[39mDOTALL\n\u001b[0;32m     83\u001b[0m ):\n",
            "\u001b[1;31mOutputParserException\u001b[0m: Could not parse LLM output: `1. Title: \"Deep Learning for Time Series Forecasting: A Review\"\n       Authors: M. S. S. Murthy, S. S. S. Prasad, and S. S. S. Prasanna\n       Description: This paper reviews the recent developments in deep learning techniques for time series forecasting. It discusses various deep learning architectures, their applications, and the challenges in implementing them for time series forecasting.\n\n    2. Title: \"Artificial Intelligence in Healthcare: Current Applications and Future Prospects\"\n       Authors: S. K. Sharma, S. K. Singh, and R. K. Pal\n       Description: This review article discusses the current applications of artificial intelligence (AI) in healthcare and its future prospects. It covers various AI techniques, their applications in healthcare, and the challenges and opportunities in implementing them.\n\n    3. Title: \"Artificial Intelligence in Education:`\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[45], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     48\u001b[0m topic \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124martificial intelligence\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28mprint\u001b[39m(fetch_recent_publications(topic))\n",
            "Cell \u001b[1;32mIn[45], line 44\u001b[0m, in \u001b[0;36mfetch_recent_publications\u001b[1;34m(topic)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch_recent_publications\u001b[39m(topic: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     43\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Query the agent to find recent scientific publications on the given topic.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m     result \u001b[38;5;241m=\u001b[39m agent_executor\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m\"\u001b[39m: topic, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent_scratchpad\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain\\chains\\base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain\\chains\\base.py:160\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 160\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    166\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    167\u001b[0m     )\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain\\agents\\agent.py:1624\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m   1622\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m-> 1624\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_take_next_step(\n\u001b[0;32m   1625\u001b[0m         name_to_tool_map,\n\u001b[0;32m   1626\u001b[0m         color_mapping,\n\u001b[0;32m   1627\u001b[0m         inputs,\n\u001b[0;32m   1628\u001b[0m         intermediate_steps,\n\u001b[0;32m   1629\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m   1630\u001b[0m     )\n\u001b[0;32m   1631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m   1632\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[0;32m   1633\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[0;32m   1634\u001b[0m         )\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain\\agents\\agent.py:1332\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1322\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1323\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1327\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1328\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m   1330\u001b[0m         [\n\u001b[0;32m   1331\u001b[0m             a\n\u001b[1;32m-> 1332\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[0;32m   1333\u001b[0m                 name_to_tool_map,\n\u001b[0;32m   1334\u001b[0m                 color_mapping,\n\u001b[0;32m   1335\u001b[0m                 inputs,\n\u001b[0;32m   1336\u001b[0m                 intermediate_steps,\n\u001b[0;32m   1337\u001b[0m                 run_manager,\n\u001b[0;32m   1338\u001b[0m             )\n\u001b[0;32m   1339\u001b[0m         ]\n\u001b[0;32m   1340\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langchain\\agents\\agent.py:1369\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1367\u001b[0m     raise_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_error:\n\u001b[1;32m-> 1369\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1370\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn output parsing error occurred. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to pass this error back to the agent and have it try \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magain, pass `handle_parsing_errors=True` to the AgentExecutor. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1373\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is the error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1374\u001b[0m     )\n\u001b[0;32m   1375\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
            "\u001b[1;31mValueError\u001b[0m: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `1. Title: \"Deep Learning for Time Series Forecasting: A Review\"\n       Authors: M. S. S. Murthy, S. S. S. Prasad, and S. S. S. Prasanna\n       Description: This paper reviews the recent developments in deep learning techniques for time series forecasting. It discusses various deep learning architectures, their applications, and the challenges in implementing them for time series forecasting.\n\n    2. Title: \"Artificial Intelligence in Healthcare: Current Applications and Future Prospects\"\n       Authors: S. K. Sharma, S. K. Singh, and R. K. Pal\n       Description: This review article discusses the current applications of artificial intelligence (AI) in healthcare and its future prospects. It covers various AI techniques, their applications in healthcare, and the challenges and opportunities in implementing them.\n\n    3. Title: \"Artificial Intelligence in Education:`\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE "
          ]
        }
      ],
      "source": [
        "from langchain.agents import create_react_agent, AgentExecutor\n",
        "from langchain.tools import Tool\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.utilities import SerpAPIWrapper\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os\n",
        "\n",
        "# Set up your API keys\n",
        "os.environ[\"SERPAPI_API_KEY\"] = creds[\"SERP_API_KEY\"]\n",
        "\n",
        "# Initialize the search tool\n",
        "search_tool = Tool(\n",
        "    name=\"Web Search\",\n",
        "    func=SerpAPIWrapper().run,\n",
        "    description=\"Search the web for recent  publications on a given topic.\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Create the ReAct agent prompt\n",
        "template = PromptTemplate(\n",
        "    input_variables=[\"topic\", \"tools\", \"tool_names\", \"agent_scratchpad\"],\n",
        "    template=\"\"\"\n",
        "    You are an AI agent that searches for recent scientific publications, blogs, news on a given topic.\n",
        "    Use the tools available to find the 5 most recent publications on {topic}.\n",
        "    Provide:\n",
        "    - Title\n",
        "    - Authors\n",
        "    - A brief description\n",
        "    \n",
        "    Available tools: {tools}\n",
        "    Tool names: {tool_names}\n",
        "    \n",
        "    {agent_scratchpad}\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Create the ReAct agent\n",
        "agent = create_react_agent(gemma, [search_tool], template)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=[search_tool], verbose=True)\n",
        "\n",
        "def fetch_recent_publications(topic: str):\n",
        "    \"\"\"Query the agent to find recent scientific publications on the given topic.\"\"\"\n",
        "    result = agent_executor.invoke({\"topic\": topic, \"agent_scratchpad\": \"\"})\n",
        "    return result\n",
        "\n",
        "# Example usage\n",
        "topic = \"artificial intelligence\"\n",
        "print(fetch_recent_publications(topic))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.tools import Tool\n",
        "\n",
        "from langchain_core.tools import Tool\n",
        "from langchain.agents import load_tools\n",
        "\n",
        "# Load tools\n",
        "arxiv_tool = load_tools([\"arxiv\"])[0]  # Extract the first tool from the list\n",
        "# google_scholar_tool = load_tools([\"google-scholar\"])[0]  # Extract the first tool from the list\n",
        "\n",
        "# toolGS = GoogleScholarQueryRun(api_wrapper=GoogleScholarAPIWrapper())\n",
        "# toolGS.run(\"LLM Models\")\n",
        "# tools.append(Tool(name=\"google-scholar\", description=\"Use it to find scientific publications in Google Scholar\", func=toolGS.run))\n",
        "# Define tools with callable functions\n",
        "\n",
        "from langchain_community.utilities.arxiv import ArxivAPIWrapper\n",
        "arxiv = ArxivAPIWrapper(\n",
        "    top_k_results = 3,\n",
        "    ARXIV_MAX_QUERY_LENGTH = 200,\n",
        "    load_max_docs = 3,\n",
        "    load_all_available_meta = False,\n",
        "    doc_content_chars_max = 400\n",
        ")\n",
        "\n",
        "\n",
        "tools = [\n",
        "    Tool(name=\"arxiv\", description=\"Use it to find scientific publications in arXiv, beforehead provide ONLY topic name\", func=arxiv.run),\n",
        "    #extract topic tool\n",
        "    Tool(name=\"ddg-search\", description=\"Use it to find scientific publications in DuckDuckGo\", func=DuckDuckGoSearchRun().run),\n",
        "    # Tool(name=\"google-scholar\", description=\"Use it to find scientific publications in Google Scholar\", func=toolGS.run),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Published: 2024-06-12\n",
            "Title: Explainable AI improves task performance in human-AI collaboration\n",
            "Authors: Julian Senoner, Simon Schallmoser, Bernhard Kratzwald, Stefan Feuerriegel, Torbj√∏rn Netland\n",
            "Summary: Artificial intelligence (AI) provides considerable opportunities to assist\n",
            "human work. However, one crucial challenge of human-AI collaboration is that\n",
            "many AI algorithms operate in a black-box \n"
          ]
        }
      ],
      "source": [
        "print(arxiv.run(\"Explainable AI\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Tool(name='arxiv', description='Use it to find scientific publications in arXiv, beforehead provide ONLY topic name', func=<bound method ArxivAPIWrapper.run of ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=3, ARXIV_MAX_QUERY_LENGTH=200, continue_on_failure=False, load_max_docs=3, load_all_available_meta=False, doc_content_chars_max=400)>),\n",
              " Tool(name='ddg-search', description='Use it to find scientific publications in DuckDuckGo', func=<bound method BaseTool.run of DuckDuckGoSearchRun(api_wrapper=DuckDuckGoSearchAPIWrapper(region='wt-wt', safesearch='moderate', time='y', max_results=5, backend='auto', source='text'))>)]"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "60jZEitmcCiP"
      },
      "outputs": [],
      "source": [
        "agent = create_react_agent(gemma, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=3, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=4000))"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "arxiv_tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(prompt.template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "question =\"Search for scientific publications on the topic of 'Explainable AI' in online databases and academic sources. Provide 3 publication on this topic. Provide a list of publications, including the title, authors, and a brief summary of each paper. Provide any related publication, first which you found\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I think I should use the arxiv tool to find scientific publications on the topic of 'Explainable AI' in online databases and academic sources. Provide 3 publication on this topic. Provide a list of publications, including the title, authors, and a brief summary of each paper. Provide any related publication, first which you found\n",
            "Action: arxiv\n",
            "Action Input: \"Explainable AI\"\n",
            "Observation\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36;1m\u001b[1;3mNo good Arxiv Result was found\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Final Answer:<eos>\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': \"Search for scientific publications on the topic of 'Explainable AI' in online databases and academic sources. Provide 3 publication on this topic. Provide a list of publications, including the title, authors, and a brief summary of each paper. Provide any related publication, first which you found\",\n",
              " 'output': '<eos>'}"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_executor.invoke({'input': question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.tools import Tool\n",
        "from langchain.utilities import ArxivAPIWrapper\n",
        "from langchain.agents import create_react_agent, AgentExecutor\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Initialize Arxiv Wrapper\n",
        "arxiv = ArxivAPIWrapper(\n",
        "    top_k_results=3,\n",
        "    ARXIV_MAX_QUERY_LENGTH=400,\n",
        "    load_max_docs=3,\n",
        "    load_all_available_meta=False,\n",
        "    doc_content_chars_max=4000\n",
        ")\n",
        "\n",
        "# Define a Tool for Arxiv search\n",
        "arxiv_tool = Tool(\n",
        "    name=\"Arxiv Search\",\n",
        "    func=arxiv.run,\n",
        "    description=\"Searches for scientific papers on Arxiv based on a query. \"\n",
        "                \"Provide a research topic or keywords to retrieve relevant papers.\"\n",
        "                \"As Output, Provide Only list of 2 publications, including the title, authors, and a brief summary of each paper.\"\n",
        ")\n",
        "\n",
        "# Define your LLM (Example: Gemma from Hugging Face)\n",
        "gemma = HuggingFaceEndpoint(\n",
        "    repo_id=\"google/gemma-7b\",\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=200\n",
        ")\n",
        "\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    You are an AI agent that answers user queries using available tools. As Output, Provide Only list of 2 publications, including the title, authors, and a brief summary of each paper.\n",
        "      \n",
        "    Available tools:\n",
        "    {tools}\n",
        "\n",
        "    Tool names: {tool_names}\n",
        "\n",
        "    User query: {input}\n",
        "\n",
        "    Thought: Let's reason step by step.\n",
        "    {agent_scratchpad}\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Create the agent with the correct prompt\n",
        "tools = [arxiv_tool]\n",
        "agent = create_react_agent(gemma, tools, prompt)\n",
        "# agent = create_react_agent(llm=gemma, tools=tools, pprompt_template)\n",
        "# Create agent executor\n",
        "# agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True  # Allow retrying on parsing errors\n",
        ")\n",
        "\n",
        "# Run the agent with a query\n",
        "query = \"Explainable AI\"\n",
        "response = agent_executor.invoke({\"input\": query})\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "\n",
            "    Paper title: Explainable AI: A Survey\n",
            "    Paper authors: Sameera Ekanayake, S. C. Abeyratne, R. Abeyratne, M. M. M. R. M. Madushanka, M. M. M. M. Madushanka\n",
            "    Paper summary: Explainable AI (XAI) is a research area that aims to provide explanations for the decisions made by AI systems. The need for XAI has increased due to the growing use of AI systems in critical decision-making domains such as healthcare and finance. However, the field of XAI is relatively new, and there is a lack of a comprehensive survey of the research in this area. This paper provides a comprehensive survey of the research in XAI, covering topics such as explainability methods, evaluation metrics, and applications. The paper also\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "    \n",
            "Observation\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "    \n",
            "    \n",
            "Observation\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "    \n",
            "    \n",
            "Observation\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "    \n",
            "Observation\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "    \n",
            "Observation\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "Observation\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "    \n",
            "    \n",
            "Observation\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "    \n",
            "    \n",
            "Observation\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "    \n",
            "Observation\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "    \n",
            "Observation\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "    \n",
            "    \n",
            "Observation\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "    \n",
            "    \n",
            "Observation\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "    \n",
            "    \n",
            "Observation\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Programs\\anaconda\\envs\\ml_course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m(\"paper authors\" - \"Author names\"),\n",
            "    (\"paper summary\" - \"Summary of the research paper\")\n",
            "    \n",
            "Observation\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'\u001b[32;1m\u001b[1;3m\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'input': 'Explainable AI', 'output': 'Agent stopped due to iteration limit or time limit.'}\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.agents import create_react_agent, AgentExecutor\n",
        "\n",
        "\n",
        "# Define the prompt template with structured output format instructions\n",
        "# prompt_template = f\"\"\"\n",
        "# Find two research papers on 'Explainable AI'. Provide only a list of two publications with:\n",
        "# 1. **Title**\n",
        "# 2. **Authors**\n",
        "# 3. **Brief summary** of each paper.\n",
        "\n",
        "# Ensure the output follows this format:\n",
        "# (name=\"title\", description=\"Title of the research paper\"),\n",
        "#     (name=\"authors\", description=\"List of authors of the research paper\"),\n",
        "#     (name=\"summary\", description=\"Brief summary of the research paper\"),\n",
        "# \"\"\"\n",
        "prompt = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    You are an AI agent that answers user queries using available tools. \n",
        "    Available tools:\n",
        "    {tools}\n",
        "\n",
        "    Tool names: {tool_names}\n",
        "\n",
        "    User query: {input}\n",
        "\n",
        "    Thought: Let's reason step by step.\n",
        "    {agent_scratchpad}\n",
        "            Ensure the output follows this format and include name of the paper, authors and summary of the paper:\n",
        "(\"paper title\" - \"Title of the research paper\"),\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Define a tool (Arxiv Search or any other search mechanism you'd like to use)\n",
        "class ArxivTool(Tool):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"Arxiv Search\", func=self.arxiv_search, description=\"Search research papers on Arxiv\")\n",
        "        \n",
        "    def arxiv_search(self, query: str):\n",
        "        # This function simulates an arxiv search, you can connect it to actual Arxiv API\n",
        "        return \"Paper 1: Explainable AI: Concepts, Applications, and Challenges. Paper 2: Explainable AI: From Black Box to Glass Box.\"\n",
        "    \n",
        "arxiv_tool = ArxivTool()\n",
        "\n",
        "# Create the agent with the correct prompt and tool\n",
        "tools = [arxiv_tool]\n",
        "agent = create_react_agent(gemma, tools, prompt)\n",
        "\n",
        "# Create the agent executor with error handling for parsing errors\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True  # Allow agent to retry if parsing fails\n",
        ")\n",
        "\n",
        "# Run the agent with a query\n",
        "query = \"Explainable AI\"\n",
        "response = agent_executor.invoke({\"input\": query})\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOqujC6qY_NY"
      },
      "source": [
        "\n",
        "\n",
        "### –ó–∞–≤–¥–∞–Ω–Ω—è 4: –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∞–≥–µ–Ω—Ç–∞-–ø–æ–º—ñ—á–Ω–∏–∫–∞ –¥–ª—è –≤–∏—Ä—ñ—à–µ–Ω–Ω—è –±—ñ–∑–Ω–µ—Å-–∑–∞–¥–∞—á\n",
        "\n",
        "–°—Ç–≤–æ—Ä—ñ—Ç—å –∞–≥–µ–Ω—Ç–∞, —è–∫–∏–π –¥–æ–ø–æ–º–∞–≥–∞—î –≤–∏—Ä—ñ—à—É–≤–∞—Ç–∏ –∑–∞–¥–∞—á—ñ –±—ñ–∑–Ω–µ—Å-–∞–Ω–∞–ª—ñ—Ç–∏–∫–∏. –ê–≥–µ–Ω—Ç –º–∞—î –¥–æ–ø–æ–º–æ–≥—Ç–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—É —Å—Ç–≤–æ—Ä–∏—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑ –ø–æ –ø—Ä–æ–¥–∞–∂–∞–º –Ω–∞ –Ω–∞—Å—Ç—É–ø–Ω–∏–π —Ä—ñ–∫ –≤—Ä–∞—Ö–æ–≤—É—é—á–∏ —Ä—ñ–≤–µ–Ω—å —ñ–Ω—Ñ–ª—è—Ü—ñ—ó —ñ –ø–æ–≥–æ–¥–Ω—ñ —É–º–æ–≤–∏. –ê–≥–µ–Ω—Ç –º–∞—î –≤–º—ñ—Ç–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Python —ñ —Ö–æ–¥–∏—Ç–∏ –≤ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç –∞–±–∏ –æ—Ç—Ä–∏–º–∞—Ç–∏ –∞–∫—Ç—É–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ.\n",
        "\n",
        "**–ö—Ä–æ–∫–∏:**\n",
        "1. –ù–∞–ª–∞—à—Ç—É–π—Ç–µ –∞–≥–µ–Ω—Ç–∞, —è–∫–∏–π –ø—Ä–∞—Ü—é–≤–∞—Ç–∏–º–µ –∑ –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏, –∑–∞–¥–∞–Ω–∏–º–∏ —Ç–µ–∫—Å—Ç–æ–º. –ö–æ—Ä–∏—Å—Ç—É–≤–∞—á –ø–∏—à–µ\n",
        "\n",
        "```\n",
        "–ú–∏ –µ–∫—Å–ø–æ—Ä—Ç—É—î–º–æ –∞–ø–µ–ª—å—Å–∏–Ω–∏ –∑ –ë—Ä–∞–∑–∏–ª—ñ—ó. –í 2021 –µ–∫—Å–ø–æ—Ä—Ç—É–≤–∞–ª–∏ 200—Ç, –≤ 2022 - 190—Ç, –≤ 2023 - 210—Ç, –≤ 2024 —è–∫–∏–π —â–µ –Ω–µ –∑–∞–∫—ñ–Ω—á–∏–≤—Å—è - 220—Ç. –ó—Ä–æ–±–∏ –æ—Ü—ñ–Ω–∫—É —Å–∫—ñ–ª—å–∫–∏ –º–∏ –∑–º–æ–∂–µ–º–æ –µ–∫—Å–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ –∞–ø–µ–ª—å—Å–∏–Ω—ñ–≤ –≤ 2025 –≤—Ä–∞—Ö–æ–≤—É—é—á–∏ –ø–æ–≥–æ–¥–Ω—ñ —É–º–æ–≤–∏ –≤ –ë—Ä–∞–∑–∏–ª—ñ—ó —ñ –ø–æ–ø–∏—Ç –Ω–∞ –∞–ø–µ–ª—å—Å–∏–Ω–∏ –≤ —Å–≤—ñ—Ç—ñ –≤–∏—Ö–æ–¥—è—á–∏ –∑ –µ–∫–æ–Ω–æ–º—ñ—á–Ω–æ—ó —Å–∏—Ç—É–∞—Ü—ñ—ó.\n",
        "```\n",
        "\n",
        "2. –°—Ç–≤–æ—Ä—ñ—Ç—å –∑–∞–ø–∏—Ç –¥–æ –∞–≥–µ–Ω—Ç–∞, —â–æ –º—ñ—Å—Ç–∏—Ç—å —á—ñ—Ç–∫–µ –∑–∞–≤–¥–∞–Ω–Ω—è ‚Äì –≤–∏–¥–∞—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±—ñ–∑–Ω–µ—Å –∞–Ω–∞–ª—ñ–∑—É –∞–±–æ –Ω–∞–ø–∏—Å–∞—Ç–∏, —â–æ –≤—ñ–Ω –Ω–µ –º–æ–∂–µ —Ü—å–æ–≥–æ –∑—Ä–æ–±–∏—Ç–∏ —ñ –∑–∞–ø–∏—Ç –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ (–ø—Ä–æ—Å—Ç–æ –º–æ–∂–µ –±—É—Ç–∏ –≤—Å–µ –æ–¥–Ω–∏–º –ø–æ–≤—ñ–¥–æ–º–ª–ª–µ–Ω–Ω—è–º).\n",
        "\n",
        "3. –ó–∞–ø—É—Å—Ç—ñ—Ç—å –∞–≥–µ–Ω—Ç–∞ —ñ –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏. –©–æ –º–æ–∂–Ω–∞ –ø–æ–∫—Ä–∞—â–∏—Ç–∏?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "UK6KA9PyRS0L"
      },
      "outputs": [],
      "source": [
        "python_repl = PythonREPL()\n",
        "python_tool = Tool(\n",
        "    name=\"python_repl\",\n",
        "    description=\"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you necessarily should print it out with `print(...)`. Otherwise you won't see the result! It's very important.\",\n",
        "    func=python_repl.run,\n",
        ")\n",
        "python_tool.name = \"python_interpreter\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('creds.json') as file:\n",
        "  creds = json.load(file)\n",
        "os.environ[\"OPENAI_API_KEY\"] = creds[\"OPENAI_API_KEY\"]\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Olha Shaposhnyk\\anaconda3\\envs\\ml_course\\Lib\\site-packages\\langsmith\\client.py:277: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"SERPAPI_API_KEY\"] = creds[\"SERPAPI_API_KEY\"]\n",
        "tools = load_tools([\"serpapi\"], llm=llm)\n",
        "tools.append(python_tool)\n",
        "prompt = hub.pull(\"hwchase17/react\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_variables=['agent_scratchpad', 'input', 'tool_names', 'tools'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'react', 'lc_hub_commit_hash': 'd15fe3c426f1c4b3f37c9198853e4a86e20c425ca7f4752ec0c9b0e97ca7ea4d'} template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}'\n"
          ]
        }
      ],
      "source": [
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent = create_react_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo estimate the export of oranges from Brazil in 2025, we need to consider both historical data and external factors such as weather conditions and global demand. Here's how we can approach this:\n",
            "\n",
            "1. **Historical Data Analysis**: We have the export data for the past four years:\n",
            "   - 2021: 200 tonnes\n",
            "   - 2022: 190 tonnes\n",
            "   - 2023: 210 tonnes\n",
            "   - 2024: 220 tonnes (projected)\n",
            "\n",
            "   We can calculate the average growth rate over these years to project the 2025 exports.\n",
            "\n",
            "2. **Weather Conditions**: Brazil's weather conditions can significantly impact orange production. We need to check the current weather forecasts and any reports on how they might affect agriculture.\n",
            "\n",
            "3. **Global Demand**: The demand for oranges can be influenced by economic conditions, health trends, and trade policies. We should look for recent reports or forecasts on global demand for oranges.\n",
            "\n",
            "Let's start by calculating the average growth rate of orange exports over the past years.\n",
            "\n",
            "Action: python_interpreter\n",
            "Action Input: \n",
            "\"\"\"\n",
            "# Calculate the average growth rate of orange exports\n",
            "exports = [200, 190, 210, 220]\n",
            "growth_rates = [(exports[i] - exports[i-1]) / exports[i-1] for i in range(1, len(exports))]\n",
            "average_growth_rate = sum(growth_rates) / len(growth_rates)\n",
            "print(average_growth_rate)\n",
            "\"\"\"\u001b[0m\u001b[33;1m\u001b[1;3m0.034294068504594814\n",
            "\u001b[0m\u001b[32;1m\u001b[1;3mThe average growth rate of orange exports from 2021 to 2024 is approximately 3.43%. Now, let's consider the external factors:\n",
            "\n",
            "1. **Weather Conditions**: We need to check the current weather forecasts and any reports on how they might affect orange production in Brazil.\n",
            "\n",
            "2. **Global Demand**: We should look for recent reports or forecasts on global demand for oranges.\n",
            "\n",
            "Let's start by searching for information on the current weather conditions in Brazil and how they might affect orange production. \n",
            "\n",
            "Action: Search\n",
            "Action Input: \"Brazil weather conditions 2024 orange production impact\"\u001b[0m\u001b[36;1m\u001b[1;3m[' entity_type: related_questions.', 'According to the business magazine ‚ÄúExame‚Äù in. November 2024, Brazil was responsible for 34 percent of the global orange production and 75 ...', 'Dry weather has reduced fruit production for five consecutive seasons in the citrus belt, which is composed of the northwest of S√£o Paulo ...', \"Brazil's orange production is expected to hit its lowest level in more than three decades in 2024/25, research center Fundecitrus said on ...\", 'Regionally, the North, Northwest, Central and South sectors all are expected to have lower orange production for the 2024/25 season according to Fundecitrus, ...', 'The weather conditions forecasted in May for the first four months of the crop season were worse than expected, with rainfall 31% lower, ...', 'Since October 2024, rains have been more regular, improving flowering. Orange trees are perennial plants that fluctuate production between ...', \"The 2023‚Äì2024 initial orange forecast for Brazil's citrus belt is 309.34 million boxes. ... Total production is forecast to be 228.52 million ...\", 'Ayres said the first point is climate conditions, with heat waves and low rainfall. \"Only 36% of mature trees are irrigated, so with a drought ...', 'The first half of 2023 saw ample rainfall, but a precipitation deficit ensued in the subsequent six months, persisting until the end of the season in 2024.', 'Weather Impact: Less rainfall and higher temperatures continued the adverse trends from the latter half of the previous year, leading to smaller-sized fruits ...']\u001b[0m\u001b[32;1m\u001b[1;3mThe search results indicate that Brazil's orange production in 2024/25 is expected to be significantly impacted by adverse weather conditions, including dry weather and lower rainfall. This has led to reduced fruit production for several consecutive seasons, and the 2024/25 season is expected to have the lowest production in more than three decades.\n",
            "\n",
            "Given these challenging weather conditions, it is likely that orange production in Brazil will be lower than usual, which could affect the export volume in 2025.\n",
            "\n",
            "Next, let's look for information on the global demand for oranges to understand how it might influence Brazil's exports in 2025.\n",
            "\n",
            "Action: Search\n",
            "Action Input: \"global demand for oranges 2025 forecast\"\u001b[0m\u001b[36;1m\u001b[1;3mOrange Market Size 2025 And Growth Rate The orange market size has grown strongly in recent years. It will grow from $3.64 billion in 2024 to $3.87 billion in 2025 at a compound annual growth rate (CAGR) of 6.3%.\u001b[0m\u001b[32;1m\u001b[1;3mThe global demand for oranges is expected to grow from $3.64 billion in 2024 to $3.87 billion in 2025, with a compound annual growth rate (CAGR) of 6.3%. This indicates a strong demand for oranges in the global market, which could positively influence Brazil's export potential despite the challenging weather conditions.\n",
            "\n",
            "Considering both the historical growth rate of Brazil's orange exports (approximately 3.43%) and the external factors:\n",
            "\n",
            "1. **Adverse Weather Conditions**: These are likely to reduce Brazil's orange production in 2025, potentially limiting the export volume.\n",
            "2. **Strong Global Demand**: This could drive higher exports if Brazil can meet the demand despite production challenges.\n",
            "\n",
            "Given these factors, it is reasonable to estimate that Brazil's orange exports in 2025 might not grow significantly due to production constraints but could still see a slight increase or stabilization due to strong global demand. A conservative estimate would be a slight increase or stabilization around the 220 tonnes projected for 2024, possibly reaching around 225 tonnes if production issues are somewhat mitigated.\n",
            "\n",
            "Final Answer: Brazil's orange exports in 2025 are estimated to be around 225 tonnes, considering adverse weather conditions and strong global demand.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'input': 'We export oranges from Brazil. In 2021, we exported 200 tonnes, in 2022 - 190 tonnes, in 2023 - 210 tonnes, in 2024, which has not yet ended - 220 tonnes. Make an estimate of how much we can export oranges in 2025, given the weather conditions in Brazil and the demand for oranges in the world based on the economic situation.', 'output': \"Brazil's orange exports in 2025 are estimated to be around 225 tonnes, considering adverse weather conditions and strong global demand.\"}\n"
          ]
        }
      ],
      "source": [
        "question = \"We export oranges from Brazil. In 2021, we exported 200 tonnes, in 2022 - 190 tonnes, in 2023 - 210 tonnes, in 2024, which has not yet ended - 220 tonnes. Make an estimate of how much we can export oranges in 2025, given the weather conditions in Brazil and the demand for oranges in the world based on the economic situation.\"\n",
        "\n",
        "# Pass the question as a dictionary with the key \"input\"\n",
        "response = agent_executor.invoke({\"input\": question})\n",
        "\n",
        "# Print the response\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input': 'We export oranges from Brazil. In 2021, we exported 200 tonnes, in 2022 - 190 tonnes, in 2023 - 210 tonnes, in 2024, which has not yet ended - 220 tonnes. Make an estimate of how much we can export oranges in 2025, given the weather conditions in Brazil and the demand for oranges in the world based on the economic situation.',\n",
              " 'output': \"Brazil's orange exports in 2025 are estimated to be around 225 tonnes, considering adverse weather conditions and strong global demand.\"}"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo estimate the export of oranges from Brazil in 2025, I would need to consider several factors, including historical export data, weather conditions in Brazil, and global demand for oranges based on the economic situation. I have historical data for the years 2021 to 2024, but I need to gather information on the current weather conditions in Brazil and the global demand for oranges.\n",
            "\n",
            "Action: Search\n",
            "Action Input: \"Brazil weather conditions 2024 impact on orange production\"\u001b[0m\u001b[36;1m\u001b[1;3mDespite this decrease, Brazil's production is anticipated to recover slightly from the current season, which was the lowest since 1988. Post forecasts orange weight at 160 grams/5.64 ounces in MY 2024/25, three percent lighter than Post's previous estimate of 165 grams for MY 2023/24 due to adverse weather conditions.\u001b[0m\u001b[32;1m\u001b[1;3mThe weather conditions in Brazil have been challenging, leading to a decrease in orange production. However, there is an expectation of a slight recovery in production. This suggests that while weather conditions have been adverse, they may not significantly hinder production in 2025.\n",
            "\n",
            "Next, I need to gather information on the global demand for oranges based on the current economic situation.\n",
            "\n",
            "Action: Search\n",
            "Action Input: \"global demand for oranges 2024 economic situation\"\u001b[0m\u001b[36;1m\u001b[1;3mOranges Market size will be USD 3915.2 million in 2024 and will expand at a compound annual growth rate (CAGR) of 4.80% from 2024 to 2031. The base year for the calculation is 2024. The historical will be 2021 to 2024. The year 2025 will be estimated one while the forecasted data will be from year 2025 to 2033.\u001b[0m\u001b[32;1m\u001b[1;3mThe global demand for oranges is expected to grow at a compound annual growth rate (CAGR) of 4.80% from 2024 to 2031. This indicates a positive trend in demand, which could support an increase in exports if production conditions allow.\n",
            "\n",
            "Given the historical export data, the challenging but slightly recovering weather conditions in Brazil, and the growing global demand for oranges, I can make an estimate for 2025.\n",
            "\n",
            "To estimate the export for 2025, I will consider the average growth in exports from 2021 to 2024 and adjust it based on the expected recovery in production and the positive demand trend.\n",
            "\n",
            "Action: python_interpreter\n",
            "Action Input: \"\"\"\n",
            "# Historical export data\n",
            "exports = [200, 190, 210, 220]\n",
            "\n",
            "# Calculate the average annual growth rate from 2021 to 2024\n",
            "growth_rates = [(exports[i] - exports[i-1]) / exports[i-1] for i in range(1, len(exports))]\n",
            "average_growth_rate = sum(growth_rates) / len(growth_rates)\n",
            "\n",
            "# Adjust the growth rate for 2025 considering the slight recovery in production and positive demand trend\n",
            "adjusted_growth_rate = average_growth_rate + 0.02  # Adding 2% for recovery and demand\n",
            "\n",
            "# Estimate exports for 2025\n",
            "estimated_exports_2025 = exports[-1] * (1 + adjusted_growth_rate)\n",
            "print(estimated_exports_2025)\n",
            "\"\"\"\u001b[0m\u001b[33;1m\u001b[1;3m231.94469507101084\n",
            "\u001b[0m\u001b[32;1m\u001b[1;3mThe estimated export of oranges from Brazil in 2025, considering the historical data, slight recovery in production due to weather conditions, and the positive trend in global demand, is approximately 232 tonnes.\n",
            "\n",
            "Final Answer: The estimated export of oranges from Brazil in 2025 is approximately 232 tonnes.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'input': 'We export oranges from Brazil. In 2021, we exported 200 tonnes, in 2022 - 190 tonnes, in 2023 - 210 tonnes, in 2024, which has not yet ended - 220 tonnes. Make an estimate of how much we can export oranges in 2025, given the weather conditions in Brazil and the demand for oranges in the world based on the economic situation. If you cannot perform this analysis, explicitly state that you cannot do so.', 'output': 'The estimated export of oranges from Brazil in 2025 is approximately 232 tonnes.'}\n"
          ]
        }
      ],
      "source": [
        "question = (\n",
        "    \"We export oranges from Brazil. In 2021, we exported 200 tonnes, in 2022 - 190 tonnes, \"\n",
        "    \"in 2023 - 210 tonnes, in 2024, which has not yet ended - 220 tonnes. \"\n",
        "    \"Make an estimate of how much we can export oranges in 2025, given the weather conditions in Brazil \"\n",
        "    \"and the demand for oranges in the world based on the economic situation. \"\n",
        "    \"If you cannot perform this analysis, explicitly state that you cannot do so.\"\n",
        ")\n",
        "\n",
        "# Pass the question to the agent\n",
        "response = agent_executor.invoke({\"input\": question})\n",
        "\n",
        "# Print the response\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input': 'We export oranges from Brazil. In 2021, we exported 200 tonnes, in 2022 - 190 tonnes, in 2023 - 210 tonnes, in 2024, which has not yet ended - 220 tonnes. Make an estimate of how much we can export oranges in 2025, given the weather conditions in Brazil and the demand for oranges in the world based on the economic situation. If you cannot perform this analysis, explicitly state that you cannot do so.',\n",
              " 'output': 'The estimated export of oranges from Brazil in 2025 is approximately 232 tonnes.'}"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml_course",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
