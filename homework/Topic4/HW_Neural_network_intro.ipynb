{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbLHTNfSclli"
      },
      "source": [
        "**Секція 1. Логістична регресія з нуля.**\n",
        "\n",
        "Будемо крок за кроком будувати модель лог регресії з нуля для передбачення, чи буде врожай більше за 80 яблук (задача подібна до лекційної, але на класифікацію).\n",
        "\n",
        "Давайте нагадаємо основні формули для логістичної регресії.\n",
        "\n",
        "### Функція гіпотези - обчислення передбачення у логістичній регресії:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ \\hat{y} $ — це ймовірність \"позитивного\" класу.\n",
        "- $ x $ — це вектор (або матриця для набору прикладів) вхідних даних.\n",
        "- $ W $ — це вектор (або матриця) вагових коефіцієнтів моделі.\n",
        "- $ b $ — це зміщення (bias).\n",
        "- $ \\sigma(z) $ — це сигмоїдна функція активації.\n",
        "\n",
        "### Як обчислюється сигмоїдна функція:\n",
        "\n",
        "Сигмоїдна функція $ \\sigma(z) $ має вигляд:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Ця функція перетворює будь-яке дійсне значення $ z $ в інтервал від 0 до 1, що дозволяє інтерпретувати вихід як ймовірність для логістичної регресії.\n",
        "\n",
        "### Формула функції втрат для логістичної регресії (бінарна крос-ентропія):\n",
        "\n",
        "Функція втрат крос-ентропії оцінює, наскільки добре модель передбачає класи, порівнюючи передбачені ймовірності $ \\hat{y} $ із справжніми мітками $ y $. Формула наступна:\n",
        "\n",
        "$$\n",
        "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
        "- $ \\hat{y} $ — це передбачене значення (ймовірність).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtOYB-RHfc_r"
      },
      "source": [
        "1.\n",
        "Тут вже наведений код для ініціювання набору даних в форматі numpy. Перетворіть `inputs`, `targets` на `torch` тензори. Виведіть результат на екран."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1936,
      "metadata": {
        "id": "3BNXSR-VdYKQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1937,
      "metadata": {
        "id": "QLKZ77x4v_-v"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 73.,  67.,  43.],\n",
            "        [ 91.,  88.,  64.],\n",
            "        [ 87., 134.,  58.],\n",
            "        [102.,  43.,  37.],\n",
            "        [ 69.,  96.,  70.]])\n",
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n"
          ]
        }
      ],
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')\n",
        "\n",
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n",
        "print(inputs)\n",
        "print(targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKzbJKfOgGV8"
      },
      "source": [
        "2. Ініціюйте ваги `w`, `b` для моделі логістичної регресії потрібної форми зважаючи на розмірності даних випадковими значеннями з нормального розподілу. Лишаю тут код для фіксації `random_seed`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1938,
      "metadata": {
        "id": "aXhKw6Tdj1-d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([5, 3]), torch.Size([5, 1]))"
            ]
          },
          "execution_count": 1938,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs.shape, targets.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1939,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 3])"
            ]
          },
          "execution_count": 1939,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1940,
      "metadata": {
        "id": "eApcB7eb6h9o"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W: tensor([[0.6614],\n",
            "        [0.2669],\n",
            "        [0.0617]], requires_grad=True)\n",
            "b: tensor([0.6213], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "# x * W + b -> [5, 3] * [3, 1] + [1] -> [5, 1]\n",
        "torch.random.manual_seed(1)\n",
        "w = torch.randn(3, 1, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "print(f'W: {w}')\n",
        "print(f'b: {b}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYGxNGTaf5s6"
      },
      "source": [
        "3. Напишіть функцію `model`, яка буде обчислювати функцію гіпотези в логістичній регресії і дозволяти робити передбачення на основі введеного рядка даних і коефіцієнтів в змінних `w`, `b`.\n",
        "\n",
        "  **Важливий момент**, що функція `model` робить обчислення на `torch.tensors`, тож для математичних обчислень використовуємо фукнціонал `torch`, наприклад:\n",
        "  - обчсилення $e^x$: `torch.exp(x)`\n",
        "  - обчсилення $log(x)$: `torch.log(x)`\n",
        "  - обчислення середнього значення вектору `x`: `torch.mean(x)`\n",
        "\n",
        "  Використайте функцію `model` для обчислення передбачень з поточними значеннями `w`, `b`.Виведіть результат обчислень на екран.\n",
        "\n",
        "  Проаналізуйте передбачення. Чи не викликають вони у вас підозр? І якщо викликають, то чим це може бути зумовлено?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1941,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model(x, w, b):\n",
        "    sigmoid = 1 / (1 + torch.exp(-(x @ w + b)))\n",
        "    return sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1942,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.]], grad_fn=<MulBackward0>)"
            ]
          },
          "execution_count": 1942,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds = model(inputs, w, b) \n",
        "preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1943,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.]], grad_fn=<MulBackward0>),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.]]))"
            ]
          },
          "execution_count": 1943,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds, targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "чому не ймовірності? - нкеоптиальна кривизка сигмоїди"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2AGM0Mb2yHa"
      },
      "source": [
        "4. Напишіть функцію `binary_cross_entropy`, яка приймає на вхід передбачення моделі `predicted_probs` та справжні мітки в даних `true_labels` і обчислює значення втрат (loss)  за формулою бінарної крос-ентропії для кожного екземпляра та вертає середні втрати по всьому набору даних.\n",
        "  Використайте функцію `binary_cross_entropy` для обчислення втрат для поточних передбачень моделі."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1944,
      "metadata": {
        "id": "1bWlovvx6kZS"
      },
      "outputs": [],
      "source": [
        "def binary_cross_entropy(y_pred, y_true):\n",
        "    y_pred = y_pred.clamp(min=1e-7, max=1-1e-7) # avoid log(0)\n",
        "    L = y_true*torch.log(y_pred) + (1 - y_true)*torch.log(1 - y_pred)\n",
        "    return (-L).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1945,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(6.3770, grad_fn=<MeanBackward0>)"
            ]
          },
          "execution_count": 1945,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss = binary_cross_entropy(preds, targets)\n",
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFKpQxdHi1__"
      },
      "source": [
        "5. Зробіть зворотнє поширення помилки і виведіть градієнти за параметрами `w`, `b`. Проаналізуйте їх значення. Як гадаєте, чому вони саме такі?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1946,
      "metadata": {
        "id": "YAbXUNSJ6mCl"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[0.],\n",
              "         [0.],\n",
              "         [0.]]),\n",
              " tensor([0.]))"
            ]
          },
          "execution_count": 1946,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss.backward()\n",
        "w.grad, b.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDN1t1RujQsK"
      },
      "source": [
        "**Що сталось?**\n",
        "\n",
        "В цій задачі, коли ми ініціювали значення випадковими значеннями з нормального розподілу, насправді ці значення не були дуже гарними стартовими значеннями і привели до того, що градієнти стали дуже малими або навіть рівними нулю (це призводить до того, що градієнти \"зникають\"), і відповідно при оновленні ваг у нас не буде нічого змінюватись. Це називається `gradient vanishing`. Це відбувається через **насичення сигмоїдної функції активації.**\n",
        "\n",
        "У нашій задачі ми використовуємо сигмоїдну функцію активації, яка має такий вигляд:\n",
        "\n",
        "   $$\n",
        "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "   $$\n",
        "\n",
        "\n",
        "Коли значення $z$ дуже велике або дуже мале, сигмоїдна функція починає \"насичуватись\". Це означає, що для великих позитивних $z$ сигмоїда наближається до 1, а для великих негативних — до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля (бо градієнт - це похідна, похідна на проміжку функції, де вона паралельна осі ОХ, дорівнює 0), що робить оновлення ваг неможливим.\n",
        "\n",
        "![](https://editor.analyticsvidhya.com/uploads/27889vaegp.png)\n",
        "\n",
        "У логістичній регресії $ z = x \\cdot w + b $. Якщо ваги $w, b$ - великі, значення $z$ також буде великим, і сигмоїда перейде в насичену область, де градієнти дуже малі.\n",
        "\n",
        "Саме це сталося в нашій задачі, де великі випадкові значення ваг викликали насичення сигмоїдної функції. Це в свою чергу призводить до того, що під час зворотного поширення помилки (backpropagation) модель оновлює ваги дуже повільно або зовсім не оновлює. Це називається проблемою **зникнення градієнтів** (gradient vanishing problem).\n",
        "\n",
        "**Що ж робити?**\n",
        "Ініціювати ваги маленькими значеннями навколо нуля. Наприклад ми можемо просто в існуючій ініціалізації ваги розділити на 1000. Можна також використати інший спосіб ініціалізації вагів - інформація про це [тут](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/).\n",
        "\n",
        "Як це робити - показую нижче. **Виконайте код та знову обчисліть передбачення, лосс і виведіть градієнти.**\n",
        "\n",
        "А я пишу пояснення, чому просто не зробити\n",
        "\n",
        "```\n",
        "w = torch.randn(1, 3, requires_grad=True)/1000\n",
        "b = torch.randn(1, requires_grad=True)/1000\n",
        "```\n",
        "\n",
        "Нам потрібно, аби тензори вагів були листовими (leaf tensors).\n",
        "\n",
        "1. **Що таке листовий тензор**\n",
        "Листовий тензор — це тензор, який був створений користувачем безпосередньо і з якого починається обчислювальний граф. Якщо такий тензор має `requires_grad=True`, PyTorch буде відслідковувати всі операції, виконані над ним, щоб правильно обчислювати градієнти під час навчання.\n",
        "\n",
        "2. **Чому ми використовуємо `w.data` замість звичайних операцій**\n",
        "Якщо ми просто виконали б операції, такі як `(w - 0.5) / 100`, ми б отримали **новий тензор**, який вже не був би листовим тензором, оскільки ці операції створюють **новий** тензор, а не модифікують існуючий.\n",
        "\n",
        "  Проте, щоб залишити наші тензори ваги `w` та зміщення `b` листовими і продовжити можливість відстеження градієнтів під час тренування, ми використовуємо атрибут `.data`. Цей атрибут дозволяє **виконувати операції in-place (прямо на існуючому тензорі)** без зміни самого об'єкта тензора. Отже, тензор залишається листовим, і PyTorch може коректно обчислювати його градієнти.\n",
        "\n",
        "3. **Чому важливо залишити тензор листовим**\n",
        "Якщо тензор більше не є листовим (наприклад, через проведення операцій, що створюють нові тензори), ви не зможете отримати градієнти за допомогою `w.grad` чи `b.grad` після виклику `loss.backward()`. Це може призвести до втрати можливості оновлення параметрів під час тренування моделі. В нашому випадку ми хочемо, щоб тензори `w` та `b` накопичували градієнти, тому вони повинні залишатись листовими.\n",
        "\n",
        "**Висновок:**\n",
        "Ми використовуємо `.data`, щоб виконати операції зміни значень на ваги і зміщення **in-place**, залишаючи їх листовими тензорами, які можуть накопичувати градієнти під час навчання. Це дозволяє коректно працювати механізму зворотного поширення помилки (backpropagation) і оновлювати ваги моделі."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOPSQyttpVjO"
      },
      "source": [
        "5. Виконайте код та знову обчисліть передбачення, лосс і знайдіть градієнти та виведіть всі ці тензори на екран."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1947,
      "metadata": {
        "id": "-EBOJ3tsnRaD"
      },
      "outputs": [],
      "source": [
        "torch.random.manual_seed(1)\n",
        "w = torch.randn(3, 1, requires_grad=True)  # Листовий тензор w = torch.randn(1, 3, requires_grad=True) \n",
        "b = torch.randn(1, requires_grad=True)     # Листовий тензор\n",
        "\n",
        "# in-place операції\n",
        "w.data = w.data / 1000\n",
        "b.data = b.data / 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1948,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[6.6135e-04],\n",
              "         [2.6692e-04],\n",
              "         [6.1677e-05]], requires_grad=True),\n",
              " tensor([0.0006], requires_grad=True))"
            ]
          },
          "execution_count": 1948,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w, b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1949,
      "metadata": {
        "id": "-JwXiSpX6orh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.5174],\n",
            "        [0.5220],\n",
            "        [0.5244],\n",
            "        [0.5204],\n",
            "        [0.5190]], grad_fn=<MulBackward0>)\n",
            "tensor(0.6829, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ],
      "source": [
        "preds_2 = model(inputs, w, b)\n",
        "print(preds_2)\n",
        "\n",
        "loss_2 = binary_cross_entropy(preds_2, targets)\n",
        "print(loss_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1950,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[ -5.4417],\n",
              "         [-18.9853],\n",
              "         [-10.0682]]),\n",
              " tensor([-0.0794]))"
            ]
          },
          "execution_count": 1950,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss_2.backward() \n",
        "w.grad, b.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCdi44IT334o"
      },
      "source": [
        "6. Напишіть алгоритм градієнтного спуску, який буде навчати модель з використанням написаних раніше функцій і виконуючи оновлення ваг. Алгоритм має включати наступні кроки:\n",
        "\n",
        "    1. Генерація прогнозів\n",
        "    2. Обчислення втрат\n",
        "    3. Обчислення градієнтів (gradients) loss-фукнції відносно ваг і зсувів\n",
        "    4. Налаштування ваг шляхом віднімання невеликої величини, пропорційної градієнту (`learning_rate` домножений на градієнт)\n",
        "    5. Скидання градієнтів на нуль\n",
        "\n",
        "Виконайте градієнтний спуск протягом 1000 епох, обчисліть фінальні передбачення і проаналізуйте, чи вони точні?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1951,
      "metadata": {
        "id": "mObHPyE06qsO"
      },
      "outputs": [],
      "source": [
        "def gradient(w, b, X, y, learning_rate = 0.001, epoch = 1000):\n",
        "    for i in range (epoch):\n",
        "        preds = model(X, w, b)\n",
        "        loss = binary_cross_entropy(preds, y)\n",
        "        loss.backward()\n",
        "        with torch.no_grad():\n",
        "            w -= w.grad * learning_rate\n",
        "            b -= b.grad * learning_rate\n",
        "\n",
        "            w.grad.zero_()\n",
        "            b.grad.zero_()\n",
        "\n",
        "        if (i % 100 == 0):   \n",
        "            print(f'epoch {i}: loss = {loss}')  \n",
        "\n",
        "    print(f'epoch {i}: loss = {loss}') # last epoch loss\n",
        "    return w, b  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1952,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0: loss = 0.6829456686973572\n",
            "epoch 100: loss = 0.1976310759782791\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 200: loss = 0.16219036281108856\n",
            "epoch 300: loss = 0.13796377182006836\n",
            "epoch 400: loss = 0.11963770538568497\n",
            "epoch 500: loss = 0.10523231327533722\n",
            "epoch 600: loss = 0.09364274144172668\n",
            "epoch 700: loss = 0.08415224403142929\n",
            "epoch 800: loss = 0.07626529037952423\n",
            "epoch 900: loss = 0.06962653994560242\n",
            "epoch 999: loss = 0.06402736902236938\n"
          ]
        }
      ],
      "source": [
        "w_final, b_final = gradient(w, b, inputs, targets, learning_rate = 0.001, epoch = 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1953,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output: \n",
            "tensor([[1.6794e-01],\n",
            "        [8.7485e-01],\n",
            "        [9.9769e-01],\n",
            "        [2.3414e-07],\n",
            "        [9.9999e-01]], grad_fn=<MulBackward0>)\n",
            "\n",
            "Rounded output: \n",
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]], grad_fn=<RoundBackward0>)\n",
            "\n",
            "Target: \n",
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "\n",
            "accuracy = 1.0\n"
          ]
        }
      ],
      "source": [
        "preds_final = model(inputs, w_final, b_final)\n",
        "print(f'Output: \\n{preds_final}\\n')\n",
        "preds_round = preds_final.round()\n",
        "print(f'Rounded output: \\n{preds_round}\\n')\n",
        "print(f'Target: \\n{targets}\\n')\n",
        "\n",
        "accuracy = (preds_round == targets).float().mean()  \n",
        "print(f'accuracy = {accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuRhlyF9qAia"
      },
      "source": [
        "**Секція 2. Створення лог регресії з використанням функціоналу `torch.nn`.**\n",
        "\n",
        "Давайте повторно реалізуємо ту ж модель, використовуючи деякі вбудовані функції та класи з PyTorch.\n",
        "\n",
        "Даних у нас буде побільше - тож, визначаємо нові масиви."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1954,
      "metadata": {
        "id": "IX8Bhm74rV4M"
      },
      "outputs": [],
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7X2dV30KtAPu"
      },
      "source": [
        "7. Завантажте вхідні дані та мітки в PyTorch тензори та з них створіть датасет, який поєднує вхідні дані з мітками, використовуючи клас `TensorDataset`. Виведіть перші 3 елементи в датасеті.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1955,
      "metadata": {
        "id": "chrvMfBs6vjo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([15, 3]) torch.Size([15, 1])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 87., 134.,  58.]]),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [1.]]))"
            ]
          },
          "execution_count": 1955,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch.nn as nn   \n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n",
        "print(inputs.shape, targets.shape)\n",
        "\n",
        "train_ds = TensorDataset(inputs, targets)\n",
        "train_ds[0:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nMFaa8suOd3"
      },
      "source": [
        "8. Визначте data loader з класом **DataLoader** для підготовленого датасету `train_ds`, встановіть розмір батчу на 5 та увімкніть перемішування даних для ефективного навчання моделі. Виведіть перший елемент в дата лоадері."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1956,
      "metadata": {
        "id": "ZCsRo5Mx6wEI"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[tensor([[ 69.,  96.,  70.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 69.,  96.,  70.],\n",
              "         [ 87., 134.,  58.],\n",
              "         [ 73.,  67.,  43.]]),\n",
              " tensor([[1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [0.]])]"
            ]
          },
          "execution_count": 1956,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_size = 5\n",
        "torch.random.manual_seed(1)\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "next(iter(train_dl))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymcQOo_hum6I"
      },
      "source": [
        "9. Створіть клас `LogReg` для логістичної регресії, наслідуючи модуль `torch.nn.Module` за прикладом в лекції (в частині про FeedForward мережі).\n",
        "\n",
        "  У нас модель складається з лінійної комбінації вхідних значень і застосування фукнції сигмоїда. Тож, нейромережа буде складатись з лінійного шару `nn.Linear` і використання активації `nn.Sigmid`. У створеному класі мають бути реалізовані методи `__init__` з ініціалізацією шарів і метод `forward` для виконання прямого проходу моделі через лінійний шар і функцію активації.\n",
        "\n",
        "  Створіть екземпляр класу `LogReg` в змінній `model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1957,
      "metadata": {
        "id": "EyAwhTBW6xxz"
      },
      "outputs": [],
      "source": [
        "class LogReg (nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(3, 1)\n",
        "        self.act = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.act(x)\n",
        "        return x    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1958,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogReg(\n",
            "  (linear): Linear(in_features=3, out_features=1, bias=True)\n",
            "  (act): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = LogReg()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RflV7xeVyoJy"
      },
      "source": [
        "10. Задайте оптимізатор `Stockastic Gradient Descent` в змінній `opt` для навчання моделі логістичної регресії. А також визначіть в змінній `loss` функцію втрат `binary_cross_entropy` з модуля `torch.nn.functional` для обчислення втрат моделі. Обчисліть втрати для поточних передбачень і міток, а потім виведіть їх. Зробіть висновок, чи моделі вдалось навчитись?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1959,
      "metadata": {
        "id": "3QCATPU_6yfa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(9.2144, grad_fn=<BinaryCrossEntropyBackward0>)"
            ]
          },
          "execution_count": 1959,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "opt = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.functional.binary_cross_entropy\n",
        "\n",
        "loss_init = loss_fn(model(inputs), targets) # first initialization of the model\n",
        "loss_init"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch-WrYnKzMzq"
      },
      "source": [
        "11. Візьміть з лекції функцію для тренування моделі з відстеженням значень втрат і навчіть щойно визначену модель на 1000 епохах. Виведіть після цього графік зміни loss, фінальні передбачення і значення таргетів."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1960,
      "metadata": {
        "id": "cEHQH9qE626k"
      },
      "outputs": [],
      "source": [
        "def fit_return_loss(num_epochs, model, loss_fn, opt, train_dl):\n",
        "    losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for xb, yb in train_dl:\n",
        "\n",
        "            pred = model(xb)\n",
        "            loss = loss_fn(pred, yb)\n",
        "\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        if (epoch % 100 == 0):   \n",
        "            print(f'epoch {epoch}: loss = {loss}')     \n",
        "\n",
        "        avg_loss = total_loss / len(train_dl)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "    print(f'epoch {i}: loss = {loss}') # last epoch loss\n",
        "          \n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1961,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0: loss = 1.9916292428970337\n",
            "epoch 100: loss = 0.11025043576955795\n",
            "epoch 200: loss = 0.08742951601743698\n",
            "epoch 300: loss = 0.08423562347888947\n",
            "epoch 400: loss = 0.056801341474056244\n",
            "epoch 500: loss = 0.021527431905269623\n",
            "epoch 600: loss = 0.06468220055103302\n",
            "epoch 700: loss = 0.015324476175010204\n",
            "epoch 800: loss = 0.038393765687942505\n",
            "epoch 900: loss = 6.565875979495828e-18\n",
            "epoch 999: loss = 0.03812045976519585\n"
          ]
        }
      ],
      "source": [
        "loss_all = fit_return_loss(1000, model, loss_fn, opt, train_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1962,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(loss_all)\n",
        "plt.title('Loss vs Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy = 1.0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(tensor([[0],\n",
              "         [1],\n",
              "         [1],\n",
              "         [0],\n",
              "         [1],\n",
              "         [0],\n",
              "         [1],\n",
              "         [1],\n",
              "         [0],\n",
              "         [1],\n",
              "         [0],\n",
              "         [1],\n",
              "         [1],\n",
              "         [0],\n",
              "         [1]], dtype=torch.int32),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.]]))"
            ]
          },
          "execution_count": 1969,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds = model(inputs)\n",
        "accuracy = (preds > 0.5).int() == targets.int()\n",
        "accuracy = accuracy.float().sum() / len(accuracy)\n",
        "print(f'accuracy = {accuracy}')\n",
        "(preds > 0.5).int(), targets  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
